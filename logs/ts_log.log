2022-06-12T01:53:26,828 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T01:53:26,828 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T01:53:26,854 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T01:53:26,854 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T01:53:27,028 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T01:53:27,028 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T01:53:27,035 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: pytorch_cnn.mar
2022-06-12T01:53:27,035 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: pytorch_cnn.mar
2022-06-12T01:53:27,122 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T01:53:27,122 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T01:53:27,122 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T01:53:27,122 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T01:53:27,122 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T01:53:27,122 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T01:53:27,122 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T01:53:27,122 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T01:53:27,130 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T01:53:27,130 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T01:53:27,130 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T01:53:27,130 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T01:53:27,177 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T01:53:27,177 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T01:53:27,178 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T01:53:27,178 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T01:53:27,179 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T01:53:27,179 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T01:53:27,179 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T01:53:27,179 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T01:53:27,179 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T01:53:27,179 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T01:53:27,313 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T01:53:27,313 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T01:53:27,355 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:53:27,355 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:53:27,617 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T01:53:27,618 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]1948
2022-06-12T01:53:27,618 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T01:53:27,619 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T01:53:27,619 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T01:53:27,619 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T01:53:27,624 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T01:53:27,624 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T01:53:27,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T01:53:27,633 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1654998807633
2022-06-12T01:53:27,633 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1654998807633
2022-06-12T01:53:27,666 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T01:53:27,749 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-06-12T01:53:27,749 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-06-12T01:53:27,750 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T01:53:27,750 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T01:53:27,750 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:624|#Level:Host|#hostname:pop-os,timestamp:1654998807
2022-06-12T01:53:27,751 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:34|#Level:Host|#hostname:pop-os,timestamp:1654998807
2022-06-12T01:54:27,355 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:54:27,355 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:55:27,355 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:55:27,355 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:56:27,355 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:56:27,355 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:57:08,682 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33206 "GET /ping HTTP/1.1" 200 6
2022-06-12T01:57:08,683 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1654999028
2022-06-12T01:57:27,355 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:57:27,355 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:58:27,356 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:58:27,356 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:59:27,352 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T01:59:27,352 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:00:27,353 [ERROR] Thread-8 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:00:27,353 [ERROR] Thread-8 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:01:27,354 [ERROR] Thread-9 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:01:27,354 [ERROR] Thread-9 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:01:54,542 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1654999314542
2022-06-12T02:01:54,542 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1654999314542
2022-06-12T02:01:54,544 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1654999314
2022-06-12T02:01:54,545 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:01:54,545 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:01:54,545 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/bbab0d1bcb0c4c94b922a5586b1ab601/model_handler.py", line 76, in handle
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/bbab0d1bcb0c4c94b922a5586b1ab601/model_handler.py", line 40, in preprocess
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data)
2022-06-12T02:01:54,546 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/json/__init__.py", line 341, in loads
2022-06-12T02:01:54,547 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     raise TypeError(f'the JSON object must be str, bytes or bytearray, '
2022-06-12T02:01:54,547 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: the JSON object must be str, bytes or bytearray, not list
2022-06-12T02:01:54,547 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33208 "POST /predictions/miso_price HTTP/1.1" 503 20
2022-06-12T02:01:54,548 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1654999028
2022-06-12T02:01:54,549 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218708, Inference time ns: 6596054
2022-06-12T02:01:54,549 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218708, Inference time ns: 6596054
2022-06-12T02:01:54,549 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:pop-os,timestamp:1654999314
2022-06-12T02:02:27,357 [ERROR] Thread-10 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:02:27,357 [ERROR] Thread-10 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:03:27,355 [ERROR] Thread-11 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:03:27,355 [ERROR] Thread-11 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:04:27,354 [ERROR] Thread-12 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:04:27,354 [ERROR] Thread-12 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:05:27,353 [ERROR] Thread-13 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:05:27,353 [ERROR] Thread-13 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:06:27,355 [ERROR] Thread-14 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:06:27,355 [ERROR] Thread-14 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:07:27,354 [ERROR] Thread-15 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:07:27,354 [ERROR] Thread-15 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:08:27,355 [ERROR] Thread-16 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:08:27,355 [ERROR] Thread-16 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:09:27,354 [ERROR] Thread-17 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:09:27,354 [ERROR] Thread-17 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:10:27,356 [ERROR] Thread-18 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:10:27,356 [ERROR] Thread-18 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:11:27,354 [ERROR] Thread-19 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:11:27,354 [ERROR] Thread-19 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:12:27,355 [ERROR] Thread-20 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:12:27,355 [ERROR] Thread-20 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:13:27,355 [ERROR] Thread-21 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:13:27,355 [ERROR] Thread-21 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:14:27,354 [ERROR] Thread-22 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:14:27,354 [ERROR] Thread-22 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:15:27,355 [ERROR] Thread-23 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:15:27,355 [ERROR] Thread-23 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:16:27,356 [ERROR] Thread-24 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:16:27,356 [ERROR] Thread-24 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:17:27,355 [ERROR] Thread-25 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:17:27,355 [ERROR] Thread-25 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:18:27,356 [ERROR] Thread-26 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:18:27,356 [ERROR] Thread-26 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:19:27,353 [ERROR] Thread-27 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:19:27,353 [ERROR] Thread-27 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:20:27,356 [ERROR] Thread-28 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:20:27,356 [ERROR] Thread-28 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:21:27,352 [ERROR] Thread-29 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:21:27,352 [ERROR] Thread-29 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:22:27,355 [ERROR] Thread-30 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:22:27,355 [ERROR] Thread-30 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:23:27,354 [ERROR] Thread-31 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:23:27,354 [ERROR] Thread-31 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:23:58,024 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:23:58,024 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:23:58,054 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:23:58,054 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:23:58,208 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612022353731-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:23:58,208 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612022353731-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:23:58,217 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612022353731-shutdown.cfg",
  "modelCount": 1,
  "created": 1655000633731,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:23:58,217 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612022353731-shutdown.cfg",
  "modelCount": 1,
  "created": 1655000633731,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:23:58,224 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612022353731-shutdown.cfg
2022-06-12T02:23:58,224 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612022353731-shutdown.cfg
2022-06-12T02:23:58,224 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612022353731-shutdown.cfg validated successfully
2022-06-12T02:23:58,224 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612022353731-shutdown.cfg validated successfully
2022-06-12T02:23:58,311 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:23:58,311 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:23:58,311 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:23:58,311 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:23:58,312 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:23:58,312 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:23:58,312 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:23:58,312 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:23:58,312 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:23:58,312 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:23:58,320 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:23:58,319 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:23:58,320 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:23:58,319 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:23:58,367 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:23:58,367 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:23:58,367 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:23:58,367 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:23:58,368 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:23:58,368 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:23:58,368 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:23:58,368 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:23:58,369 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:23:58,369 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:23:58,505 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:23:58,505 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:23:58,544 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:23:58,544 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:23:58,792 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:23:58,793 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]2479
2022-06-12T02:23:58,794 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:23:58,794 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:23:58,794 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:23:58,794 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:23:58,799 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:23:58,799 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:23:58,807 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:23:58,810 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000638810
2022-06-12T02:23:58,810 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000638810
2022-06-12T02:23:58,841 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:23:58,927 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 86
2022-06-12T02:23:58,927 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 86
2022-06-12T02:23:58,928 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:23:58,928 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:23:58,928 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:612|#Level:Host|#hostname:pop-os,timestamp:1655000638
2022-06-12T02:23:58,928 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:32|#Level:Host|#hostname:pop-os,timestamp:1655000638
2022-06-12T02:24:11,104 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33216 "GET /ping HTTP/1.1" 200 7
2022-06-12T02:24:11,105 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655000651
2022-06-12T02:24:23,249 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000663249
2022-06-12T02:24:23,249 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000663249
2022-06-12T02:24:23,251 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655000663
2022-06-12T02:24:23,252 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:24:23,252 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:24:23,252 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:24:23,252 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:24:23,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:24:23,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:24:23,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/857aa0770a244d3a9760e8c9d4ccc782/model_handler.py", line 77, in handle
2022-06-12T02:24:23,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:24:23,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/857aa0770a244d3a9760e8c9d4ccc782/model_handler.py", line 40, in preprocess
2022-06-12T02:24:23,254 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data) # returns string
2022-06-12T02:24:23,254 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/json/__init__.py", line 341, in loads
2022-06-12T02:24:23,254 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     raise TypeError(f'the JSON object must be str, bytes or bytearray, '
2022-06-12T02:24:23,254 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33218 "POST /predictions/miso_price HTTP/1.1" 503 21
2022-06-12T02:24:23,254 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: the JSON object must be str, bytes or bytearray, not list
2022-06-12T02:24:23,255 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655000651
2022-06-12T02:24:23,255 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218913, Inference time ns: 5815609
2022-06-12T02:24:23,255 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218913, Inference time ns: 5815609
2022-06-12T02:24:23,255 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:pop-os,timestamp:1655000663
2022-06-12T02:24:58,549 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:24:58,549 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:25:28,284 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000728284
2022-06-12T02:25:28,284 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000728284
2022-06-12T02:25:28,286 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655000728
2022-06-12T02:25:28,286 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:25:28,286 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:25:28,286 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:25:28,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:25:28,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:25:28,287 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33222 "POST /predictions/miso_price HTTP/1.1" 503 10
2022-06-12T02:25:28,287 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655000651
2022-06-12T02:25:28,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:25:28,287 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 146486, Inference time ns: 3322340
2022-06-12T02:25:28,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/857aa0770a244d3a9760e8c9d4ccc782/model_handler.py", line 77, in handle
2022-06-12T02:25:28,287 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 146486, Inference time ns: 3322340
2022-06-12T02:25:28,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:25:28,288 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:pop-os,timestamp:1655000728
2022-06-12T02:25:28,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/857aa0770a244d3a9760e8c9d4ccc782/model_handler.py", line 40, in preprocess
2022-06-12T02:25:28,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data) # returns string
2022-06-12T02:25:28,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/json/__init__.py", line 341, in loads
2022-06-12T02:25:28,289 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     raise TypeError(f'the JSON object must be str, bytes or bytearray, '
2022-06-12T02:25:28,289 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: the JSON object must be str, bytes or bytearray, not list
2022-06-12T02:25:58,550 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:25:58,550 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:26:58,548 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:26:58,548 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:27:00,710 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000820709
2022-06-12T02:27:00,710 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655000820709
2022-06-12T02:27:00,711 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655000820
2022-06-12T02:27:00,711 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0
2022-06-12T02:27:00,711 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0
2022-06-12T02:27:00,711 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33224 "POST /predictions/miso_price HTTP/1.1" 503 9
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655000651
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/857aa0770a244d3a9760e8c9d4ccc782/model_handler.py", line 77, in handle
2022-06-12T02:27:00,712 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 205679, Inference time ns: 3096603
2022-06-12T02:27:00,712 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:27:00,712 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 205679, Inference time ns: 3096603
2022-06-12T02:27:00,713 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/857aa0770a244d3a9760e8c9d4ccc782/model_handler.py", line 40, in preprocess
2022-06-12T02:27:00,713 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655000820
2022-06-12T02:27:00,713 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data) # returns string
2022-06-12T02:27:00,713 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/json/__init__.py", line 341, in loads
2022-06-12T02:27:00,713 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     raise TypeError(f'the JSON object must be str, bytes or bytearray, '
2022-06-12T02:27:00,713 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: the JSON object must be str, bytes or bytearray, not list
2022-06-12T02:27:58,549 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:27:58,549 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:28:58,544 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:28:58,544 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:31:38,034 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:31:38,034 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:31:38,068 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:31:38,068 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:31:38,244 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612022906675-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:31:38,244 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612022906675-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:31:38,252 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612022906675-shutdown.cfg",
  "modelCount": 1,
  "created": 1655000946675,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:31:38,252 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612022906675-shutdown.cfg",
  "modelCount": 1,
  "created": 1655000946675,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:31:38,259 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612022906675-shutdown.cfg
2022-06-12T02:31:38,259 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612022906675-shutdown.cfg
2022-06-12T02:31:38,259 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612022906675-shutdown.cfg validated successfully
2022-06-12T02:31:38,259 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612022906675-shutdown.cfg validated successfully
2022-06-12T02:31:38,343 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:31:38,343 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:31:38,343 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:31:38,343 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:31:38,344 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:31:38,344 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:31:38,344 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:31:38,344 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:31:38,344 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:31:38,344 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:31:38,352 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:31:38,352 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:31:38,352 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:31:38,352 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:31:38,399 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:31:38,399 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:31:38,399 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:31:38,399 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:31:38,400 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:31:38,400 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:31:38,400 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:31:38,400 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:31:38,401 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:31:38,401 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:31:38,534 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:31:38,534 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:31:38,583 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:31:38,583 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:31:38,806 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:31:38,808 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]2756
2022-06-12T02:31:38,808 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:31:38,808 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:31:38,809 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:31:38,809 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:31:38,814 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:31:38,814 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:31:38,822 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:31:38,824 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001098824
2022-06-12T02:31:38,824 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001098824
2022-06-12T02:31:38,858 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:31:38,942 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-06-12T02:31:38,942 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-06-12T02:31:38,943 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:31:38,943 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:31:38,943 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:595|#Level:Host|#hostname:pop-os,timestamp:1655001098
2022-06-12T02:31:38,944 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:pop-os,timestamp:1655001098
2022-06-12T02:31:44,932 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33226 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:31:44,932 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001104
2022-06-12T02:31:48,519 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001108519
2022-06-12T02:31:48,519 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001108519
2022-06-12T02:31:48,520 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001108
2022-06-12T02:31:48,522 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:31:48,522 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:31:48,523 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33228 "POST /predictions/miso_price HTTP/1.1" 503 20
2022-06-12T02:31:48,524 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001104
2022-06-12T02:31:48,524 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 216082, Inference time ns: 5671694
2022-06-12T02:31:48,524 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 216082, Inference time ns: 5671694
2022-06-12T02:31:48,524 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655001108
2022-06-12T02:31:48,526 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [{'post_data': bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')}]
2022-06-12T02:31:48,527 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:31:48,527 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:31:48,527 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:31:48,527 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:31:48,528 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/38ce67b91f4c4ef680a375445cb5d7b0/model_handler.py", line 78, in handle
2022-06-12T02:31:48,528 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:31:48,528 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/38ce67b91f4c4ef680a375445cb5d7b0/model_handler.py", line 41, in preprocess
2022-06-12T02:31:48,528 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data[0]) # returns string
2022-06-12T02:31:48,529 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/json/__init__.py", line 341, in loads
2022-06-12T02:31:48,529 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     raise TypeError(f'the JSON object must be str, bytes or bytearray, '
2022-06-12T02:31:48,529 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: the JSON object must be str, bytes or bytearray, not dict
2022-06-12T02:32:38,574 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:32:38,574 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:33:38,576 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:33:38,576 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:34:38,576 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:34:38,576 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:35:17,018 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:35:17,018 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:35:17,048 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:35:17,048 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:35:17,196 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023514860-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:35:17,196 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023514860-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:35:17,204 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023514860-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001314860,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:35:17,204 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023514860-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001314860,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:35:17,211 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023514860-shutdown.cfg
2022-06-12T02:35:17,211 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023514860-shutdown.cfg
2022-06-12T02:35:17,211 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023514860-shutdown.cfg validated successfully
2022-06-12T02:35:17,211 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023514860-shutdown.cfg validated successfully
2022-06-12T02:35:17,296 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:35:17,296 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:35:17,296 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:35:17,296 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:35:17,297 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:35:17,297 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:35:17,297 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:35:17,297 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:35:17,297 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:35:17,297 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:35:17,305 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:35:17,305 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:35:17,305 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:35:17,305 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:35:17,352 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:35:17,352 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:35:17,353 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:35:17,353 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:35:17,354 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:35:17,354 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:35:17,354 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:35:17,354 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:35:17,355 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:35:17,355 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:35:17,499 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:35:17,499 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:35:17,542 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:35:17,542 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:35:17,770 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:35:17,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]3015
2022-06-12T02:35:17,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:35:17,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:35:17,772 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:35:17,772 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:35:17,777 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:35:17,777 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:35:17,785 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:35:17,788 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001317788
2022-06-12T02:35:17,788 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001317788
2022-06-12T02:35:17,824 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:35:17,907 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-06-12T02:35:17,907 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-06-12T02:35:17,908 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:35:17,908 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:35:17,908 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:607|#Level:Host|#hostname:pop-os,timestamp:1655001317
2022-06-12T02:35:17,909 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:pop-os,timestamp:1655001317
2022-06-12T02:35:24,742 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001324742
2022-06-12T02:35:24,742 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001324742
2022-06-12T02:35:24,744 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001324
2022-06-12T02:35:24,744 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:35:24,745 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:35:24,744 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:35:24,745 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:35:24,745 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:35:24,745 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:35:24,745 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/29d891257a334776b1c983efab002794/model_handler.py", line 79, in handle
2022-06-12T02:35:24,745 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:35:24,746 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/29d891257a334776b1c983efab002794/model_handler.py", line 40, in preprocess
2022-06-12T02:35:24,746 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     post_data = f.read()
2022-06-12T02:35:24,746 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'f' is not defined
2022-06-12T02:35:24,748 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33230 "POST /predictions/miso_price HTTP/1.1" 503 22
2022-06-12T02:35:24,748 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001324
2022-06-12T02:35:24,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 148725, Inference time ns: 6786441
2022-06-12T02:35:24,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 148725, Inference time ns: 6786441
2022-06-12T02:35:24,749 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:pop-os,timestamp:1655001324
2022-06-12T02:36:11,870 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:36:11,870 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:36:11,901 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:36:11,901 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:36:12,048 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023610248-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:36:12,048 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023610248-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:36:12,058 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023610248-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001370248,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:36:12,058 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023610248-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001370248,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:36:12,064 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023610248-shutdown.cfg
2022-06-12T02:36:12,064 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023610248-shutdown.cfg
2022-06-12T02:36:12,065 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023610248-shutdown.cfg validated successfully
2022-06-12T02:36:12,065 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023610248-shutdown.cfg validated successfully
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:36:12,151 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:36:12,151 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:36:12,151 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:36:12,159 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:36:12,159 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:36:12,159 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:36:12,159 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:36:12,204 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:36:12,204 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:36:12,204 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:36:12,204 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:36:12,206 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:36:12,206 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:36:12,206 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:36:12,206 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:36:12,206 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:36:12,206 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:36:12,341 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:36:12,341 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:36:12,385 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:36:12,385 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:36:12,637 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:36:12,638 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]3259
2022-06-12T02:36:12,638 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:36:12,639 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:36:12,639 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:36:12,639 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:36:12,644 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:36:12,644 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:36:12,652 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:36:12,655 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001372655
2022-06-12T02:36:12,655 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001372655
2022-06-12T02:36:12,684 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:36:12,773 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-06-12T02:36:12,773 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-06-12T02:36:12,774 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:36:12,774 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:36:12,774 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:619|#Level:Host|#hostname:pop-os,timestamp:1655001372
2022-06-12T02:36:12,775 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:30|#Level:Host|#hostname:pop-os,timestamp:1655001372
2022-06-12T02:36:19,827 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001379827
2022-06-12T02:36:19,827 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001379827
2022-06-12T02:36:19,828 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001379
2022-06-12T02:36:19,829 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:36:19,829 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:36:19,829 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:36:19,829 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:36:19,829 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:36:19,830 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:36:19,830 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9be27c36cbbe4dc7af9c1ee27e2a52e1/model_handler.py", line 79, in handle
2022-06-12T02:36:19,830 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:36:19,830 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9be27c36cbbe4dc7af9c1ee27e2a52e1/model_handler.py", line 40, in preprocess
2022-06-12T02:36:19,830 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     post_data = post_data.read()
2022-06-12T02:36:19,831 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - AttributeError: 'list' object has no attribute 'read'
2022-06-12T02:36:19,833 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33232 "POST /predictions/miso_price HTTP/1.1" 503 27
2022-06-12T02:36:19,833 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001379
2022-06-12T02:36:19,833 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 243977, Inference time ns: 7132912
2022-06-12T02:36:19,833 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 243977, Inference time ns: 7132912
2022-06-12T02:36:19,834 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:pop-os,timestamp:1655001379
2022-06-12T02:37:12,383 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:37:12,383 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:37:30,908 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:37:30,908 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:37:30,940 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:37:30,940 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:37:31,104 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023729154-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:37:31,104 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023729154-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:37:31,112 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023729154-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001449154,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:37:31,112 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023729154-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001449154,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:37:31,119 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023729154-shutdown.cfg
2022-06-12T02:37:31,119 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023729154-shutdown.cfg
2022-06-12T02:37:31,119 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023729154-shutdown.cfg validated successfully
2022-06-12T02:37:31,119 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023729154-shutdown.cfg validated successfully
2022-06-12T02:37:31,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:37:31,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:37:31,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:37:31,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:37:31,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:37:31,204 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:37:31,204 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:37:31,204 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:37:31,205 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:37:31,205 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:37:31,214 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:37:31,214 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:37:31,216 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:37:31,216 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:37:31,263 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:37:31,263 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:37:31,263 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:37:31,263 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:37:31,264 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:37:31,264 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:37:31,264 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:37:31,264 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:37:31,265 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:37:31,265 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:37:31,402 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:37:31,402 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:37:31,444 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:37:31,444 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:37:31,674 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:37:31,675 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]3510
2022-06-12T02:37:31,675 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:37:31,676 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:37:31,676 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:37:31,676 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:37:31,681 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:37:31,681 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:37:31,689 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:37:31,691 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001451691
2022-06-12T02:37:31,691 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001451691
2022-06-12T02:37:31,727 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:37:31,811 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-06-12T02:37:31,811 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-06-12T02:37:31,812 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:37:31,812 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:37:31,812 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:602|#Level:Host|#hostname:pop-os,timestamp:1655001451
2022-06-12T02:37:31,813 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:pop-os,timestamp:1655001451
2022-06-12T02:37:37,793 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001457793
2022-06-12T02:37:37,793 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001457793
2022-06-12T02:37:37,795 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001457
2022-06-12T02:37:37,796 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:37:37,796 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:37:37,796 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:37:37,796 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:37:37,796 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:37:37,796 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:37:37,797 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/c7a1758738034f19ac4c23368a7c705f/model_handler.py", line 79, in handle
2022-06-12T02:37:37,797 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:37:37,797 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/c7a1758738034f19ac4c23368a7c705f/model_handler.py", line 40, in preprocess
2022-06-12T02:37:37,797 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     post_data = post_data[0].read()
2022-06-12T02:37:37,797 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - AttributeError: 'dict' object has no attribute 'read'
2022-06-12T02:37:37,800 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33234 "POST /predictions/miso_price HTTP/1.1" 503 23
2022-06-12T02:37:37,800 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001457
2022-06-12T02:37:37,800 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 231083, Inference time ns: 7416795
2022-06-12T02:37:37,800 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 231083, Inference time ns: 7416795
2022-06-12T02:37:37,801 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:pop-os,timestamp:1655001457
2022-06-12T02:38:31,439 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:38:31,439 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:38:48,512 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:38:48,512 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:38:48,546 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:38:48,546 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:38:48,712 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023846974-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:38:48,712 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612023846974-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:38:48,722 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023846974-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001526974,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:38:48,722 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612023846974-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001526974,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:38:48,730 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023846974-shutdown.cfg
2022-06-12T02:38:48,730 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612023846974-shutdown.cfg
2022-06-12T02:38:48,730 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023846974-shutdown.cfg validated successfully
2022-06-12T02:38:48,730 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612023846974-shutdown.cfg validated successfully
2022-06-12T02:38:48,817 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:38:48,817 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:38:48,817 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:38:48,817 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:38:48,817 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:38:48,817 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:38:48,817 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:38:48,817 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:38:48,818 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:38:48,818 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:38:48,825 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:38:48,825 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:38:48,825 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:38:48,825 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:38:48,869 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:38:48,869 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:38:48,869 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:38:48,869 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:38:48,870 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:38:48,870 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:38:48,870 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:38:48,870 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:38:48,870 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:38:48,870 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:38:49,001 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:38:49,001 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:38:49,046 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:38:49,046 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:38:49,304 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:38:49,305 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]3755
2022-06-12T02:38:49,305 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:38:49,306 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:38:49,306 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:38:49,306 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:38:49,311 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:38:49,311 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:38:49,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:38:49,322 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001529322
2022-06-12T02:38:49,322 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001529322
2022-06-12T02:38:49,351 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:38:49,435 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-06-12T02:38:49,435 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-06-12T02:38:49,436 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:38:49,436 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:38:49,436 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:615|#Level:Host|#hostname:pop-os,timestamp:1655001529
2022-06-12T02:38:49,437 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:30|#Level:Host|#hostname:pop-os,timestamp:1655001529
2022-06-12T02:38:55,066 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33236 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:38:55,067 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001535
2022-06-12T02:38:56,623 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001536623
2022-06-12T02:38:56,623 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001536623
2022-06-12T02:38:56,626 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001536
2022-06-12T02:38:56,627 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:38:56,627 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:38:56,629 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33238 "POST /predictions/miso_price HTTP/1.1" 503 22
2022-06-12T02:38:56,629 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001535
2022-06-12T02:38:56,629 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 187770, Inference time ns: 6186606
2022-06-12T02:38:56,629 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 187770, Inference time ns: 6186606
2022-06-12T02:38:56,630 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:pop-os,timestamp:1655001536
2022-06-12T02:38:56,632 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'post_data': bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')}
2022-06-12T02:38:56,633 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:38:56,633 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:38:56,633 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:38:56,634 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:38:56,634 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d6af6f53d5340e3b0dd5e27efbb51e9/model_handler.py", line 78, in handle
2022-06-12T02:38:56,634 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:38:56,634 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d6af6f53d5340e3b0dd5e27efbb51e9/model_handler.py", line 41, in preprocess
2022-06-12T02:38:56,635 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data) # returns string
2022-06-12T02:38:56,635 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/json/__init__.py", line 341, in loads
2022-06-12T02:38:56,635 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     raise TypeError(f'the JSON object must be str, bytes or bytearray, '
2022-06-12T02:38:56,635 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: the JSON object must be str, bytes or bytearray, not list
2022-06-12T02:39:49,043 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:39:49,043 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:40:17,655 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:40:17,655 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:40:17,689 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:40:17,689 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:40:17,860 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024016270-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:40:17,860 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024016270-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:40:17,865 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024016270-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001616271,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:40:17,865 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024016270-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001616271,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:40:17,871 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024016270-shutdown.cfg
2022-06-12T02:40:17,871 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024016270-shutdown.cfg
2022-06-12T02:40:17,872 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024016270-shutdown.cfg validated successfully
2022-06-12T02:40:17,872 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024016270-shutdown.cfg validated successfully
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:40:17,958 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:40:17,958 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:40:17,958 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:40:17,966 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:40:17,966 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:40:17,966 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:40:17,966 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:40:18,016 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:40:18,016 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:40:18,016 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:40:18,016 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:40:18,017 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:40:18,017 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:40:18,017 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:40:18,017 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:40:18,018 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:40:18,018 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:40:18,148 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:40:18,148 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:40:18,190 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:40:18,190 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:40:18,435 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:40:18,436 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]4016
2022-06-12T02:40:18,436 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:40:18,436 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:40:18,437 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:40:18,437 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:40:18,441 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:40:18,441 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:40:18,450 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:40:18,452 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001618452
2022-06-12T02:40:18,452 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001618452
2022-06-12T02:40:18,487 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:40:18,574 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-06-12T02:40:18,574 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-06-12T02:40:18,575 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:40:18,575 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:40:18,575 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:613|#Level:Host|#hostname:pop-os,timestamp:1655001618
2022-06-12T02:40:18,576 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:37|#Level:Host|#hostname:pop-os,timestamp:1655001618
2022-06-12T02:40:26,160 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33240 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:40:26,160 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001626
2022-06-12T02:40:27,963 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001627963
2022-06-12T02:40:27,963 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001627963
2022-06-12T02:40:27,964 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001627
2022-06-12T02:40:27,966 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:40:27,966 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:40:27,967 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33242 "POST /predictions/miso_price HTTP/1.1" 503 20
2022-06-12T02:40:27,968 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001626
2022-06-12T02:40:27,968 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 242345, Inference time ns: 5186756
2022-06-12T02:40:27,968 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 242345, Inference time ns: 5186756
2022-06-12T02:40:27,968 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655001627
2022-06-12T02:40:27,969 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'post_data': bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')}
2022-06-12T02:40:27,970 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:40:27,970 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:40:27,970 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:40:27,970 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:40:27,970 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/3a20aa82322247c997055464b7f41777/model_handler.py", line 78, in handle
2022-06-12T02:40:27,970 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:40:27,971 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/3a20aa82322247c997055464b7f41777/model_handler.py", line 41, in preprocess
2022-06-12T02:40:27,971 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data = json.loads(post_data['post_data']) # returns string
2022-06-12T02:40:27,971 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: list indices must be integers or slices, not str
2022-06-12T02:41:18,191 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:41:18,191 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:42:18,192 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:42:18,192 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:43:26,260 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:43:26,260 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:43:26,292 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:43:26,292 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:43:26,437 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024314276-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:43:26,437 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024314276-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:43:26,444 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024314276-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001794276,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:43:26,444 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024314276-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001794276,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:43:26,450 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024314276-shutdown.cfg
2022-06-12T02:43:26,450 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024314276-shutdown.cfg
2022-06-12T02:43:26,451 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024314276-shutdown.cfg validated successfully
2022-06-12T02:43:26,451 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024314276-shutdown.cfg validated successfully
2022-06-12T02:43:26,534 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:43:26,534 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:43:26,535 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:43:26,535 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:43:26,535 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:43:26,535 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:43:26,535 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:43:26,535 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:43:26,535 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:43:26,535 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:43:26,543 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:43:26,542 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:43:26,543 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:43:26,542 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:43:26,588 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:43:26,588 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:43:26,589 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:43:26,589 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:43:26,590 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:43:26,590 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:43:26,590 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:43:26,590 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:43:26,591 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:43:26,591 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:43:26,727 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:43:26,727 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:43:26,772 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:43:26,772 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:43:27,012 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:43:27,013 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]4278
2022-06-12T02:43:27,013 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:43:27,013 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:43:27,014 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:43:27,014 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:43:27,019 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:43:27,019 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:43:27,027 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:43:27,029 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001807029
2022-06-12T02:43:27,029 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001807029
2022-06-12T02:43:27,066 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:43:27,152 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-06-12T02:43:27,152 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-06-12T02:43:27,153 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:43:27,153 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:43:27,153 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:615|#Level:Host|#hostname:pop-os,timestamp:1655001807
2022-06-12T02:43:27,154 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:37|#Level:Host|#hostname:pop-os,timestamp:1655001807
2022-06-12T02:43:31,546 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33244 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:43:31,547 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001811
2022-06-12T02:43:33,317 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001813317
2022-06-12T02:43:33,317 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001813317
2022-06-12T02:43:33,318 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001813
2022-06-12T02:43:33,319 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:43:33,319 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:43:33,319 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:43:33,319 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:43:33,319 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:43:33,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:43:33,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/661953c359db4e81b89e282f77264342/model_handler.py", line 78, in handle
2022-06-12T02:43:33,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:43:33,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/661953c359db4e81b89e282f77264342/model_handler.py", line 40, in preprocess
2022-06-12T02:43:33,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     print(post_data['post_data'][0])
2022-06-12T02:43:33,321 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: list indices must be integers or slices, not str
2022-06-12T02:43:33,320 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33246 "POST /predictions/miso_price HTTP/1.1" 503 19
2022-06-12T02:43:33,321 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001811
2022-06-12T02:43:33,321 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 155599, Inference time ns: 4827427
2022-06-12T02:43:33,321 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 155599, Inference time ns: 4827427
2022-06-12T02:43:33,322 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655001813
2022-06-12T02:44:26,768 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:44:26,768 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:44:49,430 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:44:49,430 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:44:49,462 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:44:49,462 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:44:49,617 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024447487-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:44:49,617 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024447487-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:44:49,626 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024447487-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001887487,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:44:49,626 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024447487-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001887487,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:44:49,632 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024447487-shutdown.cfg
2022-06-12T02:44:49,632 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024447487-shutdown.cfg
2022-06-12T02:44:49,632 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024447487-shutdown.cfg validated successfully
2022-06-12T02:44:49,632 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024447487-shutdown.cfg validated successfully
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:44:49,718 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:44:49,718 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:44:49,718 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:44:49,726 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:44:49,727 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:44:49,727 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:44:49,726 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:44:49,773 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:44:49,773 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:44:49,774 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:44:49,774 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:44:49,775 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:44:49,775 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:44:49,775 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:44:49,775 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:44:49,775 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:44:49,775 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:44:49,906 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:44:49,906 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:44:49,949 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:44:49,949 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:44:50,181 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:44:50,182 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]4535
2022-06-12T02:44:50,182 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:44:50,183 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:44:50,183 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:44:50,183 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:44:50,186 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:44:50,186 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:44:50,193 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:44:50,195 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001890195
2022-06-12T02:44:50,195 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001890195
2022-06-12T02:44:50,229 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:44:50,311 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-06-12T02:44:50,311 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-06-12T02:44:50,312 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:44:50,312 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:44:50,312 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:590|#Level:Host|#hostname:pop-os,timestamp:1655001890
2022-06-12T02:44:50,312 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:pop-os,timestamp:1655001890
2022-06-12T02:44:56,517 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33248 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:44:56,518 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001896
2022-06-12T02:44:59,061 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001899061
2022-06-12T02:44:59,061 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001899061
2022-06-12T02:44:59,063 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001899
2022-06-12T02:44:59,063 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/e25e46097b0547feadb59d301edb27eb/model_handler.py", line 78, in handle
2022-06-12T02:44:59,064 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:44:59,065 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/e25e46097b0547feadb59d301edb27eb/model_handler.py", line 40, in preprocess
2022-06-12T02:44:59,065 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     print(post_data['post_data'])
2022-06-12T02:44:59,065 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - TypeError: list indices must be integers or slices, not str
2022-06-12T02:44:59,066 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33250 "POST /predictions/miso_price HTTP/1.1" 503 24
2022-06-12T02:44:59,066 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001896
2022-06-12T02:44:59,067 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 234330, Inference time ns: 5710967
2022-06-12T02:44:59,067 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 234330, Inference time ns: 5710967
2022-06-12T02:44:59,067 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655001899
2022-06-12T02:45:49,949 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:45:49,949 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:45:55,499 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:45:55,499 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:45:55,533 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:45:55,533 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:45:55,689 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024553803-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:45:55,689 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024553803-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:45:55,698 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024553803-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001953804,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:45:55,698 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024553803-shutdown.cfg",
  "modelCount": 1,
  "created": 1655001953804,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:45:55,704 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024553803-shutdown.cfg
2022-06-12T02:45:55,704 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024553803-shutdown.cfg
2022-06-12T02:45:55,705 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024553803-shutdown.cfg validated successfully
2022-06-12T02:45:55,705 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024553803-shutdown.cfg validated successfully
2022-06-12T02:45:55,788 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:45:55,788 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:45:55,788 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:45:55,788 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:45:55,789 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:45:55,789 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:45:55,789 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:45:55,789 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:45:55,789 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:45:55,789 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:45:55,797 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:45:55,797 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:45:55,797 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:45:55,797 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:45:55,850 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:45:55,850 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:45:55,850 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:45:55,850 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:45:55,852 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:45:55,852 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:45:55,852 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:45:55,852 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:45:55,853 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:45:55,853 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:45:55,985 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:45:55,985 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:45:56,026 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:45:56,026 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:45:56,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:45:56,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]4788
2022-06-12T02:45:56,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:45:56,295 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:45:56,295 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:45:56,295 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:45:56,300 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:45:56,300 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:45:56,309 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:45:56,311 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001956311
2022-06-12T02:45:56,311 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001956311
2022-06-12T02:45:56,344 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:45:56,427 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-06-12T02:45:56,427 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-06-12T02:45:56,428 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:45:56,428 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:45:56,428 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:635|#Level:Host|#hostname:pop-os,timestamp:1655001956
2022-06-12T02:45:56,429 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:pop-os,timestamp:1655001956
2022-06-12T02:46:00,278 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33252 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:46:00,279 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001960
2022-06-12T02:46:02,516 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001962516
2022-06-12T02:46:02,516 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655001962516
2022-06-12T02:46:02,519 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655001962
2022-06-12T02:46:02,520 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-06-12T02:46:02,520 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-06-12T02:46:02,521 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33254 "POST /predictions/miso_price HTTP/1.1" 503 21
2022-06-12T02:46:02,522 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655001960
2022-06-12T02:46:02,522 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T02:46:02,522 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218079, Inference time ns: 5968062
2022-06-12T02:46:02,522 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218079, Inference time ns: 5968062
2022-06-12T02:46:02,522 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:46:02,522 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:46:02,522 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655001962
2022-06-12T02:46:02,522 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:46:02,523 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:46:02,523 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/6faddaa7fa2141b587fd7b82636741d0/model_handler.py", line 78, in handle
2022-06-12T02:46:02,523 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:46:02,523 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/6faddaa7fa2141b587fd7b82636741d0/model_handler.py", line 44, in preprocess
2022-06-12T02:46:02,523 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     for k in post_data_loaded.keys():
2022-06-12T02:46:02,523 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'post_data_loaded' is not defined
2022-06-12T02:46:56,474 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:46:56,474 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:46:56,510 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:46:56,510 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:46:56,672 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024654461-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:46:56,672 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024654461-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:46:56,680 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024654461-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002014461,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:46:56,680 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024654461-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002014461,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:46:56,687 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024654461-shutdown.cfg
2022-06-12T02:46:56,687 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024654461-shutdown.cfg
2022-06-12T02:46:56,687 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024654461-shutdown.cfg validated successfully
2022-06-12T02:46:56,687 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024654461-shutdown.cfg validated successfully
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:46:56,774 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:46:56,774 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:46:56,774 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:46:56,782 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:46:56,781 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:46:56,782 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:46:56,781 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:46:56,828 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:46:56,828 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:46:56,828 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:46:56,828 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:46:56,829 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:46:56,829 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:46:56,829 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:46:56,829 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:46:56,830 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:46:56,830 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:46:56,959 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:46:56,959 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:46:57,004 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:46:57,004 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:46:57,257 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:46:57,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5052
2022-06-12T02:46:57,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:46:57,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:46:57,259 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:46:57,259 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:46:57,263 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:46:57,263 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:46:57,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:46:57,274 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002017274
2022-06-12T02:46:57,274 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002017274
2022-06-12T02:46:57,309 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:46:57,395 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-06-12T02:46:57,395 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-06-12T02:46:57,396 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:46:57,396 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:46:57,396 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:618|#Level:Host|#hostname:pop-os,timestamp:1655002017
2022-06-12T02:46:57,396 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:pop-os,timestamp:1655002017
2022-06-12T02:47:04,444 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002024444
2022-06-12T02:47:04,444 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002024444
2022-06-12T02:47:04,446 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655002024
2022-06-12T02:47:04,447 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-06-12T02:47:04,447 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-06-12T02:47:04,450 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33256 "POST /predictions/miso_price HTTP/1.1" 503 23
2022-06-12T02:47:04,451 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002024
2022-06-12T02:47:04,451 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T02:47:04,451 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605783, Inference time ns: 7667249
2022-06-12T02:47:04,451 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605783, Inference time ns: 7667249
2022-06-12T02:47:04,451 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:47:04,451 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:pop-os,timestamp:1655002024
2022-06-12T02:47:04,451 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:47:04,452 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:47:04,452 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:47:04,452 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2bd60cc762444897aeecac65b8ce0b29/model_handler.py", line 78, in handle
2022-06-12T02:47:04,452 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_input = self.preprocess(data)
2022-06-12T02:47:04,452 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2bd60cc762444897aeecac65b8ce0b29/model_handler.py", line 45, in preprocess
2022-06-12T02:47:04,453 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     preprocessed_data[k] = torch.asarray(preprocessed_data[k])#.to(device)
2022-06-12T02:47:04,453 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'torch' is not defined
2022-06-12T02:47:57,002 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:47:57,002 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:48:11,913 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:48:11,913 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:48:11,953 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:48:11,953 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:48:12,108 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024809750-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:48:12,108 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612024809750-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:48:12,120 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024809750-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002089750,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:48:12,120 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612024809750-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002089750,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:48:12,125 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024809750-shutdown.cfg
2022-06-12T02:48:12,125 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612024809750-shutdown.cfg
2022-06-12T02:48:12,126 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024809750-shutdown.cfg validated successfully
2022-06-12T02:48:12,126 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612024809750-shutdown.cfg validated successfully
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:48:12,211 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:48:12,211 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:48:12,211 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:48:12,219 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:48:12,219 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:48:12,220 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:48:12,220 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:48:12,266 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:48:12,266 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:48:12,266 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:48:12,266 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:48:12,267 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:48:12,267 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:48:12,267 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:48:12,267 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:48:12,268 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:48:12,268 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:48:12,402 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:48:12,402 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:48:12,446 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:48:12,446 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:48:12,684 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:48:12,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5306
2022-06-12T02:48:12,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:48:12,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:48:12,686 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:48:12,686 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:48:12,691 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:48:12,691 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:48:12,699 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:48:12,702 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002092702
2022-06-12T02:48:12,702 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002092702
2022-06-12T02:48:12,741 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:48:12,825 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-06-12T02:48:12,825 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-06-12T02:48:12,826 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:48:12,826 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:48:12,826 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:611|#Level:Host|#hostname:pop-os,timestamp:1655002092
2022-06-12T02:48:12,827 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:41|#Level:Host|#hostname:pop-os,timestamp:1655002092
2022-06-12T02:48:18,368 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33258 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:48:18,369 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002098
2022-06-12T02:48:20,892 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002100892
2022-06-12T02:48:20,892 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002100892
2022-06-12T02:48:20,894 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655002100
2022-06-12T02:48:20,895 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:48:20,895 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-06-12T02:48:20,897 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33262 "POST /predictions/miso_price HTTP/1.1" 503 21
2022-06-12T02:48:20,897 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002098
2022-06-12T02:48:20,898 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 241484, Inference time ns: 5601694
2022-06-12T02:48:20,898 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 241484, Inference time ns: 5601694
2022-06-12T02:48:20,898 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:pop-os,timestamp:1655002100
2022-06-12T02:48:20,899 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T02:48:20,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:48:20,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:48:20,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:48:20,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:48:20,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/ab2246a5e1f0439f94aab376b5cf9f90/model_handler.py", line 80, in handle
2022-06-12T02:48:20,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.inference(model_input)
2022-06-12T02:48:20,901 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/ab2246a5e1f0439f94aab376b5cf9f90/model_handler.py", line 58, in inference
2022-06-12T02:48:20,901 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.model.forward(**model_input)
2022-06-12T02:48:20,901 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - AttributeError: 'ModelHandler' object has no attribute 'model'
2022-06-12T02:49:12,445 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:49:12,445 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:50:12,444 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:50:12,444 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:51:12,445 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:51:12,445 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:51:41,475 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:51:41,475 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:51:41,517 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:51:41,517 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:51:41,661 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025140159-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:51:41,661 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025140159-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:51:41,667 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025140159-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002300160,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:51:41,667 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025140159-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002300160,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:51:41,673 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025140159-shutdown.cfg
2022-06-12T02:51:41,673 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025140159-shutdown.cfg
2022-06-12T02:51:41,674 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025140159-shutdown.cfg validated successfully
2022-06-12T02:51:41,674 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025140159-shutdown.cfg validated successfully
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:51:41,764 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:51:41,764 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:51:41,764 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:51:41,772 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:51:41,772 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:51:41,772 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:41,772 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:41,820 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:51:41,820 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:51:41,821 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:51:41,821 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:51:41,822 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:51:41,822 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:51:41,822 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:51:41,822 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:51:41,823 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:51:41,823 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:51:41,963 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:51:41,963 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:51:42,004 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:51:42,004 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:51:42,252 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:51:42,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5562
2022-06-12T02:51:42,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:51:42,253 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:51:42,254 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:51:42,254 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:51:42,258 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:42,258 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:42,266 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:51:42,268 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002302268
2022-06-12T02:51:42,268 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002302268
2022-06-12T02:51:42,299 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:51:42,399 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:51:42,399 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:51:42,400 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:51:42,400 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:51:42,400 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:51:42,400 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:51:42,400 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:42,400 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:51:42,400 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:42,401 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:51:42,401 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:51:42,401 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:51:42,402 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:51:42,402 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:51:42,402 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:42,402 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:42,402 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:51:42,403 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:51:42,403 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:51:42,403 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:42,403 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:42,414 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:42,414 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:42,414 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:42,414 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:42,415 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:42,415 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:42,415 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:42,415 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:42,415 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:51:42,415 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:51:42,428 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:42,428 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:42,428 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:42,428 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:43,416 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:43,416 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:43,886 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:51:43,887 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5621
2022-06-12T02:51:43,887 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:51:43,888 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:43,888 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:51:43,888 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:43,888 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:43,888 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:43,890 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002303890
2022-06-12T02:51:43,890 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002303890
2022-06-12T02:51:43,890 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:51:43,904 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:51:44,007 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:51:44,007 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:44,008 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:51:44,007 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:44,008 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:51:44,008 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:44,008 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:44,008 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:51:44,009 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:51:44,009 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:51:44,008 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:44,009 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:51:44,009 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:51:44,008 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:44,009 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:51:44,009 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:44,009 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:44,009 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:51:44,010 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:44,010 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:44,010 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:51:44,010 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:51:44,010 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:44,010 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:51:44,010 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:44,010 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:44,010 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:44,010 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:51:44,010 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:51:44,011 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:44,010 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:51:44,011 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:44,024 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:44,024 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:45,011 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:45,011 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:45,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:51:45,482 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5627
2022-06-12T02:51:45,482 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:51:45,482 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:51:45,482 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:45,482 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:45,483 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:45,483 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:45,485 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002305485
2022-06-12T02:51:45,485 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002305485
2022-06-12T02:51:45,485 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:51:45,498 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:51:45,604 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:51:45,604 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:51:45,604 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:51:45,604 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:51:45,604 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:51:45,605 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:51:45,605 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:51:45,605 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:51:45,605 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:45,606 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:51:45,605 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:45,606 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:51:45,606 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:45,606 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:51:45,606 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:45,606 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:51:45,606 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:45,606 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:45,606 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:51:45,607 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:45,607 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:45,607 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:45,607 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:45,607 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-12T02:51:45,607 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-12T02:51:45,622 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:45,622 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:45,622 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:45,622 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:47,608 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:47,608 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:48,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:51:48,054 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5633
2022-06-12T02:51:48,054 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:51:48,054 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:48,054 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:51:48,054 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:48,055 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:48,055 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:48,057 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002308057
2022-06-12T02:51:48,057 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:51:48,057 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002308057
2022-06-12T02:51:48,067 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:51:48,148 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:51:48,148 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:51:48,148 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:51:48,148 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:51:48,149 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:51:48,149 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:51:48,149 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:51:48,149 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:48,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:51:48,150 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:48,150 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:51:48,150 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:48,150 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:51:48,150 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:48,150 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:48,150 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:51:48,150 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:51:48,150 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-12T02:51:48,151 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:48,151 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:48,164 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:48,164 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:51,151 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:51,151 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:51,598 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:51:51,599 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5643
2022-06-12T02:51:51,599 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:51:51,599 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:51,599 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:51,599 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:51:51,599 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:51,599 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:51,601 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002311601
2022-06-12T02:51:51,601 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002311601
2022-06-12T02:51:51,601 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:51:51,614 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:51:51,719 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:51:51,719 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:51:51,719 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:51,719 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:51,719 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:51:51,720 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:51,720 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:51:51,720 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:51,720 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:51:51,720 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:51:51,720 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:51:51,720 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:51:51,720 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:51:51,720 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:51:51,721 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:51,721 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:51:51,721 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:51:51,721 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:51:51,721 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:51:51,721 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:51,721 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:51,722 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:51,722 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:51,722 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:51,722 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:51,722 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-12T02:51:51,722 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-12T02:51:51,736 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:51,736 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:56,723 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:56,723 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:51:57,164 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:51:57,164 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5651
2022-06-12T02:51:57,165 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:51:57,165 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:57,165 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:51:57,165 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:51:57,165 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:57,165 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:51:57,167 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002317167
2022-06-12T02:51:57,167 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002317167
2022-06-12T02:51:57,167 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:51:57,180 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:51:57,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:51:57,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:51:57,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:51:57,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:51:57,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:51:57,283 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:51:57,283 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:51:57,284 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:57,284 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:51:57,284 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:57,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:51:57,284 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:51:57,284 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:57,284 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:51:57,284 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:57,284 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:51:57,285 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:57,285 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:51:57,285 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:57,285 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:51:57,285 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-12T02:51:57,285 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-12T02:51:57,299 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:57,299 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:51:57,299 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:51:57,299 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:05,286 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:52:05,286 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:52:05,756 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:52:05,757 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5657
2022-06-12T02:52:05,757 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:52:05,757 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:52:05,757 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:52:05,757 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:52:05,757 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:52:05,757 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:52:05,759 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002325759
2022-06-12T02:52:05,759 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002325759
2022-06-12T02:52:05,759 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:52:05,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:52:05,876 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:52:05,876 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:52:05,876 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:52:05,876 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:52:05,876 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:52:05,876 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:52:05,876 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:52:05,877 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:52:05,877 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:52:05,877 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:52:05,877 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:52:05,877 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:52:05,877 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:52:05,877 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:52:05,877 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-12T02:52:05,877 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-12T02:52:05,893 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:52:05,893 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:52:05,893 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:05,893 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:18,878 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:52:18,878 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:52:19,343 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:52:19,344 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5663
2022-06-12T02:52:19,344 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:52:19,344 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:52:19,344 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:52:19,344 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:52:19,344 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:52:19,344 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:52:19,346 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002339346
2022-06-12T02:52:19,346 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002339346
2022-06-12T02:52:19,346 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:52:19,358 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:52:19,453 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:52:19,453 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:52:19,454 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:52:19,454 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:52:19,454 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:52:19,454 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:52:19,454 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:52:19,454 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:52:19,454 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:52:19,455 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:52:19,455 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:52:19,455 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:52:19,455 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:52:19,455 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:52:19,455 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:52:19,455 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:52:19,455 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:52:19,455 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-12T02:52:19,455 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-12T02:52:19,469 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:52:19,469 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:52:19,469 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:19,469 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:40,456 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:52:40,456 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:52:40,898 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:52:40,899 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5669
2022-06-12T02:52:40,899 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:52:40,899 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:52:40,899 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:52:40,899 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:52:40,899 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:52:40,899 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:52:40,901 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002360901
2022-06-12T02:52:40,901 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:52:40,901 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002360901
2022-06-12T02:52:40,913 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:52:41,015 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:52:41,015 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:52:41,015 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:52:41,016 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:52:41,016 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:52:41,016 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:52:41,016 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:52:41,016 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:52:41,016 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:52:41,016 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:52:41,017 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:52:41,017 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:52:41,017 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:52:41,017 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:52:41,017 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:52:41,017 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:52:41,017 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:52:41,017 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:52:41,017 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:52:41,017 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-12T02:52:41,017 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-12T02:52:41,032 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:52:41,032 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:52:41,032 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:41,032 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:52:42,005 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:52:42,005 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:53:15,018 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:53:15,018 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:53:15,478 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:53:15,478 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5677
2022-06-12T02:53:15,478 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:53:15,478 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:53:15,478 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:53:15,478 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:53:15,479 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:53:15,479 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:53:15,481 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002395481
2022-06-12T02:53:15,481 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002395481
2022-06-12T02:53:15,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:53:15,492 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:53:15,591 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:53:15,591 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:53:15,591 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:53:15,592 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:53:15,592 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:53:15,592 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:53:15,592 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:53:15,592 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:53:15,592 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:53:15,592 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:53:15,592 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:53:15,592 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:53:15,593 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:53:15,593 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:53:15,593 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:53:15,593 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:53:15,593 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-12T02:53:15,593 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-06-12T02:53:15,606 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:53:15,606 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:53:15,606 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:53:15,606 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:53:42,005 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:53:42,005 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:54:10,593 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:10,593 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:11,058 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:11,058 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5688
2022-06-12T02:54:11,059 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:11,059 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:11,059 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:11,059 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:11,059 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:11,059 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:11,061 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002451061
2022-06-12T02:54:11,061 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002451061
2022-06-12T02:54:11,061 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:11,073 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:11,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:11,176 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:11,176 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:11,176 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:11,176 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:11,176 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/9fe10af418cd4cc29230b7b6ccc57593/model_handler.py", line 43, in initialize
2022-06-12T02:54:11,176 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:11,177 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:11,176 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:11,177 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:11,177 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:11,177 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:11,177 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:11,177 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:11,177 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:11,177 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:11,177 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:11,177 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:11,177 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-12T02:54:11,177 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-06-12T02:54:11,191 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:11,191 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:11,191 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:11,191 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:20,339 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:54:20,339 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:54:20,374 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:54:20,374 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:54:20,532 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025417942-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:54:20,532 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025417942-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:54:20,538 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025417942-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002457942,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:54:20,538 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025417942-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002457942,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:54:20,544 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025417942-shutdown.cfg
2022-06-12T02:54:20,544 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025417942-shutdown.cfg
2022-06-12T02:54:20,544 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025417942-shutdown.cfg validated successfully
2022-06-12T02:54:20,544 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025417942-shutdown.cfg validated successfully
2022-06-12T02:54:20,629 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:54:20,629 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:54:20,630 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:54:20,630 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:54:20,630 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:54:20,630 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:54:20,630 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:54:20,630 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:54:20,630 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:54:20,630 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:54:20,638 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:54:20,638 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:54:20,638 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:20,638 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:20,686 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:54:20,686 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:54:20,686 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:54:20,686 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:54:20,687 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:54:20,687 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:54:20,687 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:54:20,687 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:54:20,688 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:54:20,688 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:54:20,818 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:54:20,818 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:54:20,862 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:54:20,862 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:54:21,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:21,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5894
2022-06-12T02:54:21,118 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:21,118 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:21,118 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:54:21,118 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:54:21,124 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:21,124 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:21,132 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:21,135 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002461135
2022-06-12T02:54:21,135 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002461135
2022-06-12T02:54:21,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:21,254 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:21,255 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:21,255 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:21,255 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:21,256 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:21,256 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:21,256 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:21,256 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:21,257 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:21,257 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:21,257 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:21,257 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:21,257 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:21,257 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:21,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:21,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:21,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:21,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:21,258 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:21,257 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:21,257 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:21,269 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:21,269 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:21,269 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:21,269 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:21,270 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:21,270 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:21,270 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:21,270 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:21,270 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:54:21,270 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:54:21,284 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:21,284 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:21,284 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:21,284 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:22,271 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:22,271 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:22,736 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:22,737 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5943
2022-06-12T02:54:22,737 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:22,737 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:22,737 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:22,737 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:22,737 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:22,737 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:22,739 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002462739
2022-06-12T02:54:22,739 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002462739
2022-06-12T02:54:22,740 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:22,753 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:22,832 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:22,832 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:22,832 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:22,832 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:22,832 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:22,832 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:22,833 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:22,833 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:22,833 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:22,833 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:22,833 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:22,833 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:22,833 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:22,834 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:22,833 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:22,834 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:22,834 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:22,834 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:22,834 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:22,834 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:22,834 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:22,834 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:22,834 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:22,835 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:22,835 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:22,835 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:22,835 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:22,835 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:22,835 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:54:22,835 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:22,835 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-06-12T02:54:22,835 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:22,835 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:22,848 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:22,848 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:23,836 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:23,836 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:24,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:24,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5948
2022-06-12T02:54:24,283 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:24,283 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:24,283 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:24,284 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:24,284 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:24,284 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:24,286 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002464286
2022-06-12T02:54:24,286 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002464286
2022-06-12T02:54:24,286 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:24,298 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:24,379 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:24,379 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:24,379 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:24,379 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:24,379 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:24,380 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:24,380 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:24,380 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:24,380 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:24,380 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:24,380 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:24,380 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:24,381 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:24,381 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:24,381 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:24,381 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:24,381 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-12T02:54:24,381 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-06-12T02:54:24,394 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:24,394 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:24,395 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:24,395 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:26,382 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:26,382 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:26,827 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:26,828 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5953
2022-06-12T02:54:26,828 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:26,828 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:26,828 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:26,828 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:26,828 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:26,828 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:26,830 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:26,830 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002466830
2022-06-12T02:54:26,830 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002466830
2022-06-12T02:54:26,841 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:26,916 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:26,916 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:26,917 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:26,917 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:26,917 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:26,917 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:26,917 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:26,918 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:26,918 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:26,918 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:26,918 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:26,918 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:26,918 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:26,918 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:26,918 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:26,918 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:26,918 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:26,918 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:26,918 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:26,919 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:26,919 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:26,919 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:26,919 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:26,919 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-12T02:54:26,919 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-06-12T02:54:26,933 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:26,933 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:26,933 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:26,933 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:29,919 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:29,919 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:30,385 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:30,386 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5962
2022-06-12T02:54:30,386 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:30,386 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:30,386 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:30,386 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:30,387 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:30,387 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:30,388 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002470388
2022-06-12T02:54:30,388 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002470388
2022-06-12T02:54:30,389 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:30,401 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:30,480 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:30,480 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:30,480 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:30,480 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:30,480 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:30,480 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:30,481 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:30,481 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:30,481 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:30,481 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:30,481 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:30,481 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:30,481 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:30,481 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:30,481 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:30,481 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:30,481 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:30,481 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:30,481 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:30,482 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:30,482 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-12T02:54:30,482 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:30,482 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-06-12T02:54:30,496 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:30,496 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:33,600 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33266 "GET /ping HTTP/1.1" 200 5
2022-06-12T02:54:33,601 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002473
2022-06-12T02:54:35,482 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:35,482 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:35,958 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:35,959 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5974
2022-06-12T02:54:35,959 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:35,959 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:35,959 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:35,959 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:35,959 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:35,959 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:35,961 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002475961
2022-06-12T02:54:35,961 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002475961
2022-06-12T02:54:35,961 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:35,973 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:36,052 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:36,052 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:36,052 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:36,053 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:36,053 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:36,053 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:36,053 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:36,053 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:36,054 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:36,054 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:36,054 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:36,054 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:36,054 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:36,054 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:36,054 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:36,054 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:36,054 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:36,054 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:36,054 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:36,054 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:36,054 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:36,055 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-12T02:54:36,055 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-06-12T02:54:36,067 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:36,067 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:36,067 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:36,067 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:44,055 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:44,055 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:44,534 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:44,534 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5980
2022-06-12T02:54:44,534 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:44,534 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:44,534 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:44,534 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:44,535 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:44,535 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:44,536 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002484536
2022-06-12T02:54:44,536 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002484536
2022-06-12T02:54:44,537 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:44,548 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:44,630 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:44,630 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:44,631 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:44,631 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:44,631 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:44,631 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:44,631 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:44,632 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:44,631 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:44,632 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:44,631 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:44,632 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:44,632 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:44,632 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:44,632 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:44,632 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:44,632 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:44,632 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:44,632 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:44,632 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:44,633 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-12T02:54:44,633 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-06-12T02:54:44,649 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:44,649 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:44,649 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:44,649 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:57,633 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:57,633 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:54:58,079 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:54:58,079 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5985
2022-06-12T02:54:58,079 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:54:58,079 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:54:58,080 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:58,080 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:54:58,080 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:58,080 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:54:58,082 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002498082
2022-06-12T02:54:58,082 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002498082
2022-06-12T02:54:58,082 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:54:58,094 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:54:58,174 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:54:58,174 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:54:58,174 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:54:58,174 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:54:58,174 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:58,174 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:54:58,174 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:54:58,174 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:54:58,175 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:54:58,175 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:54:58,175 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:54:58,175 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:54:58,175 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:58,175 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:54:58,175 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:54:58,175 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:58,175 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:54:58,175 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:58,175 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:54:58,175 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:58,175 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:54:58,176 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-12T02:54:58,176 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-06-12T02:54:58,189 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:58,189 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:54:58,189 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:54:58,189 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:55:19,176 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:55:19,176 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:55:19,653 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:55:19,653 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]5990
2022-06-12T02:55:19,654 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:55:19,654 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:55:19,654 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:55:19,654 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-06-12T02:55:19,654 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:55:19,654 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:55:19,656 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002519656
2022-06-12T02:55:19,656 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002519656
2022-06-12T02:55:19,656 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:55:19,669 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend worker process died.
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 210, in <module>
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     worker.run_server()
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 181, in run_server
2022-06-12T02:55:19,748 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-06-12T02:55:19,748 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 139, in handle_connection
2022-06-12T02:55:19,748 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-06-12T02:55:19,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py", line 104, in load_model
2022-06-12T02:55:19,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/model_loader.py", line 151, in load
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2d108ed4fd5941a7bd7064bfcdd108fb/model_handler.py", line 44, in initialize
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_pt_path = os.path.join(model_dir, serialized_file)
2022-06-12T02:55:19,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:55:19,749 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - NameError: name 'os' is not defined
2022-06-12T02:55:19,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-06-12T02:55:19,749 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:55:19,749 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: miso_price, error: Worker died.
2022-06-12T02:55:19,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:55:19,749 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-06-12T02:55:19,749 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:55:19,749 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stderr
2022-06-12T02:55:19,750 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:55:19,750 [WARN ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-miso_price_1.0-stdout
2022-06-12T02:55:19,750 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-12T02:55:19,750 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-06-12T02:55:19,764 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:55:19,764 [INFO ] W-9000-miso_price_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stdout
2022-06-12T02:55:19,764 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:55:19,764 [INFO ] W-9000-miso_price_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-miso_price_1.0-stderr
2022-06-12T02:55:20,858 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:55:20,858 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:55:46,131 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:55:46,131 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:55:46,166 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:55:46,166 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:55:46,321 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025532518-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:55:46,321 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025532518-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:55:46,330 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025532518-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002532518,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:55:46,330 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025532518-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002532518,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:55:46,336 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025532518-shutdown.cfg
2022-06-12T02:55:46,336 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025532518-shutdown.cfg
2022-06-12T02:55:46,337 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025532518-shutdown.cfg validated successfully
2022-06-12T02:55:46,337 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025532518-shutdown.cfg validated successfully
2022-06-12T02:55:46,421 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:55:46,421 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:55:46,421 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:55:46,421 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:55:46,421 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:55:46,421 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:55:46,422 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:55:46,422 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:55:46,422 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:55:46,422 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:55:46,429 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:55:46,429 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:55:46,429 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:55:46,429 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:55:46,476 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:55:46,476 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:55:46,477 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:55:46,477 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:55:46,478 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:55:46,478 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:55:46,478 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:55:46,478 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:55:46,478 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:55:46,478 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:55:46,612 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:55:46,612 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:55:46,654 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:55:46,654 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:55:46,888 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:55:46,889 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]6191
2022-06-12T02:55:46,890 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:55:46,890 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:55:46,890 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:55:46,890 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:55:46,894 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:55:46,894 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:55:46,901 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:55:46,904 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002546904
2022-06-12T02:55:46,904 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002546904
2022-06-12T02:55:46,940 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:55:49,044 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2104
2022-06-12T02:55:49,044 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2104
2022-06-12T02:55:49,045 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:55:49,045 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:55:49,045 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2620|#Level:Host|#hostname:pop-os,timestamp:1655002549
2022-06-12T02:55:49,046 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:pop-os,timestamp:1655002549
2022-06-12T02:55:59,204 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33270 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:55:59,205 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002559
2022-06-12T02:56:01,710 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002561709
2022-06-12T02:56:01,710 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002561709
2022-06-12T02:56:01,711 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655002561
2022-06-12T02:56:01,764 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-06-12T02:56:01,764 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-06-12T02:56:01,766 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33272 "POST /predictions/miso_price HTTP/1.1" 503 72
2022-06-12T02:56:01,767 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002559
2022-06-12T02:56:01,767 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 259349, Inference time ns: 57676366
2022-06-12T02:56:01,767 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 259349, Inference time ns: 57676366
2022-06-12T02:56:01,767 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:pop-os,timestamp:1655002561
2022-06-12T02:56:01,769 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T02:56:01,769 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:56:01,770 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:56:01,770 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:56:01,770 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:56:01,770 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2903620ac83340508238b455b68c929b/model_handler.py", line 99, in handle
2022-06-12T02:56:01,770 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.inference(model_input)
2022-06-12T02:56:01,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/2903620ac83340508238b455b68c929b/model_handler.py", line 77, in inference
2022-06-12T02:56:01,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.model.forward(**model_input)
2022-06-12T02:56:01,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: The following operation failed in the TorchScript interpreter.
2022-06-12T02:56:01,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, serialized code (most recent call last):
2022-06-12T02:56:01,771 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__.py", line 29, in forward
2022-06-12T02:56:01,772 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = getattr(hist_future_conv, "0")
2022-06-12T02:56:01,772 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(hist_future_conv, "1")
2022-06-12T02:56:01,772 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future0 = (_0).forward(hist_future, )
2022-06-12T02:56:01,772 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~~~~~~ <--- HERE
2022-06-12T02:56:01,772 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future1 = (_1).forward(hist_future0, )
2022-06-12T02:56:01,772 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_conv = self.hist_conv
2022-06-12T02:56:01,773 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/container.py", line 23, in forward
2022-06-12T02:56:01,773 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(self, "1")
2022-06-12T02:56:01,773 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _2 = getattr(self, "2")
2022-06-12T02:56:01,773 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input0 = (_0).forward(input, )
2022-06-12T02:56:01,773 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -               ~~~~~~~~~~~ <--- HERE
2022-06-12T02:56:01,773 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input1 = (_1).forward(input0, )
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return (_2).forward(input1, )
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 23, in forward
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight = self.weight
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias = self.bias
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = (self)._conv_forward(input, weight, bias, )
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           ~~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T02:56:01,774 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _0
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv1d,
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 29, in _conv_forward
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight: Tensor,
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias: Optional[Tensor]) -> Tensor:
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = torch.conv1d(input, weight, bias, [1], "same", [1])
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          ~~~~~~~~~~~~ <--- HERE
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _1
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - 
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, original code (most recent call last):
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/ipykernel_15/297113794.py", line 97, in forward
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         # forward pass through conv blocks
2022-06-12T02:56:01,775 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_future_conv:
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             hist_future = layer(hist_future)
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                           ~~~~~ <--- HERE
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_conv:
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input):
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for module in self:
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             input = module(input)
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~ <--- HERE
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return input
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 302, in forward
2022-06-12T02:56:01,776 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input: Tensor) -> Tensor:
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return self._conv_forward(input, self.weight, self.bias)
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 298, in _conv_forward
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             weight, bias, self.stride,
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             _single(0), self.dilation, self.groups)
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return F.conv1d(input, weight, bias, self.stride,
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~ <--- HERE
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                         self.padding, self.dilation, self.groups)
2022-06-12T02:56:01,777 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)
2022-06-12T02:56:46,654 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:56:46,654 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:57:36,108 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:57:36,108 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T02:57:36,143 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:57:36,143 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T02:57:36,316 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025734744-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:57:36,316 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612025734744-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T02:57:36,324 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025734744-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002654744,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:57:36,324 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612025734744-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002654744,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T02:57:36,331 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025734744-shutdown.cfg
2022-06-12T02:57:36,331 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612025734744-shutdown.cfg
2022-06-12T02:57:36,331 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025734744-shutdown.cfg validated successfully
2022-06-12T02:57:36,331 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612025734744-shutdown.cfg validated successfully
2022-06-12T02:57:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:57:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T02:57:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:57:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:57:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:57:36,414 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T02:57:36,415 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:57:36,415 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T02:57:36,415 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:57:36,415 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T02:57:36,423 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:57:36,423 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:57:36,423 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T02:57:36,423 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T02:57:36,471 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:57:36,471 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T02:57:36,471 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:57:36,471 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T02:57:36,472 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:57:36,472 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T02:57:36,472 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:57:36,472 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T02:57:36,473 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:57:36,473 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T02:57:36,607 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:57:36,607 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T02:57:36,650 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:57:36,650 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:57:36,898 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T02:57:36,899 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]6447
2022-06-12T02:57:36,899 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T02:57:36,900 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T02:57:36,900 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:57:36,900 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T02:57:36,905 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:57:36,905 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T02:57:36,919 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T02:57:36,922 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002656922
2022-06-12T02:57:36,922 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002656922
2022-06-12T02:57:36,947 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T02:57:39,205 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2259
2022-06-12T02:57:39,205 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2259
2022-06-12T02:57:39,206 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:57:39,206 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T02:57:39,206 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2788|#Level:Host|#hostname:pop-os,timestamp:1655002659
2022-06-12T02:57:39,207 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:26|#Level:Host|#hostname:pop-os,timestamp:1655002659
2022-06-12T02:57:45,919 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33274 "GET /ping HTTP/1.1" 200 6
2022-06-12T02:57:45,920 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002665
2022-06-12T02:57:49,301 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002669301
2022-06-12T02:57:49,301 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002669301
2022-06-12T02:57:49,303 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655002669
2022-06-12T02:57:49,358 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-06-12T02:57:49,358 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-06-12T02:57:49,360 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33276 "POST /predictions/miso_price HTTP/1.1" 503 74
2022-06-12T02:57:49,360 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002665
2022-06-12T02:57:49,361 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 216055, Inference time ns: 59676204
2022-06-12T02:57:49,361 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T02:57:49,361 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 216055, Inference time ns: 59676204
2022-06-12T02:57:49,361 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T02:57:49,361 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T02:57:49,361 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655002669
2022-06-12T02:57:49,361 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T02:57:49,361 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/39e6d5ae0f894abab469632603a094ff/model_handler.py", line 100, in handle
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.inference(model_input)
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/39e6d5ae0f894abab469632603a094ff/model_handler.py", line 78, in inference
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.model.forward(**model_input)
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: The following operation failed in the TorchScript interpreter.
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, serialized code (most recent call last):
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__.py", line 29, in forward
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = getattr(hist_future_conv, "0")
2022-06-12T02:57:49,362 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(hist_future_conv, "1")
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future0 = (_0).forward(hist_future, )
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~~~~~~ <--- HERE
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future1 = (_1).forward(hist_future0, )
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_conv = self.hist_conv
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/container.py", line 23, in forward
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(self, "1")
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _2 = getattr(self, "2")
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input0 = (_0).forward(input, )
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -               ~~~~~~~~~~~ <--- HERE
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input1 = (_1).forward(input0, )
2022-06-12T02:57:49,363 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return (_2).forward(input1, )
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 23, in forward
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight = self.weight
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias = self.bias
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = (self)._conv_forward(input, weight, bias, )
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           ~~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _0
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv1d,
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 29, in _conv_forward
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight: Tensor,
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias: Optional[Tensor]) -> Tensor:
2022-06-12T02:57:49,364 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = torch.conv1d(input, weight, bias, [1], "same", [1])
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          ~~~~~~~~~~~~ <--- HERE
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _1
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - 
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, original code (most recent call last):
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/ipykernel_15/297113794.py", line 97, in forward
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         # forward pass through conv blocks
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_future_conv:
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             hist_future = layer(hist_future)
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                           ~~~~~ <--- HERE
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_conv:
2022-06-12T02:57:49,365 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input):
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for module in self:
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             input = module(input)
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~ <--- HERE
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return input
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 302, in forward
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input: Tensor) -> Tensor:
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return self._conv_forward(input, self.weight, self.bias)
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 298, in _conv_forward
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             weight, bias, self.stride,
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             _single(0), self.dilation, self.groups)
2022-06-12T02:57:49,366 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return F.conv1d(input, weight, bias, self.stride,
2022-06-12T02:57:49,367 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~ <--- HERE
2022-06-12T02:57:49,367 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                         self.padding, self.dilation, self.groups)
2022-06-12T02:57:49,367 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: expected scalar type Float but found Double
2022-06-12T02:58:36,649 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:58:36,649 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:59:36,651 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T02:59:36,651 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:00:11,416 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:00:11,416 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:00:11,452 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:00:11,452 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:00:11,633 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030009465-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:00:11,633 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030009465-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:00:11,641 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030009465-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002809466,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:00:11,641 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030009465-shutdown.cfg",
  "modelCount": 1,
  "created": 1655002809466,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:00:11,647 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030009465-shutdown.cfg
2022-06-12T03:00:11,647 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030009465-shutdown.cfg
2022-06-12T03:00:11,648 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030009465-shutdown.cfg validated successfully
2022-06-12T03:00:11,648 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030009465-shutdown.cfg validated successfully
2022-06-12T03:00:11,732 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:00:11,732 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:00:11,733 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:00:11,733 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:00:11,733 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:00:11,733 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:00:11,733 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:00:11,733 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:00:11,733 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:00:11,733 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:00:11,741 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:00:11,740 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:00:11,741 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:00:11,740 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:00:11,786 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:00:11,786 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:00:11,786 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:00:11,786 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:00:11,788 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:00:11,788 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:00:11,788 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:00:11,788 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:00:11,788 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:00:11,788 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:00:11,924 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:00:11,924 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:00:11,967 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:00:11,967 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:00:12,192 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:00:12,195 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]6736
2022-06-12T03:00:12,196 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:00:12,196 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:00:12,196 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:00:12,196 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:00:12,201 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:00:12,201 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:00:12,209 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:00:12,211 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002812211
2022-06-12T03:00:12,211 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002812211
2022-06-12T03:00:12,245 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:00:14,463 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2219
2022-06-12T03:00:14,463 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2219
2022-06-12T03:00:14,464 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:00:14,464 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:00:14,464 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2727|#Level:Host|#hostname:pop-os,timestamp:1655002814
2022-06-12T03:00:14,464 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:34|#Level:Host|#hostname:pop-os,timestamp:1655002814
2022-06-12T03:00:18,146 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33278 "GET /ping HTTP/1.1" 200 6
2022-06-12T03:00:18,147 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002818
2022-06-12T03:00:20,049 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002820049
2022-06-12T03:00:20,049 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655002820049
2022-06-12T03:00:20,051 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655002820
2022-06-12T03:00:20,109 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-06-12T03:00:20,109 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-06-12T03:00:20,110 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33280 "POST /predictions/miso_price HTTP/1.1" 503 77
2022-06-12T03:00:20,111 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655002818
2022-06-12T03:00:20,111 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 231425, Inference time ns: 62065635
2022-06-12T03:00:20,111 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 231425, Inference time ns: 62065635
2022-06-12T03:00:20,111 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:pop-os,timestamp:1655002820
2022-06-12T03:00:20,111 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/89973fac539548c598001685e09e55aa/model_handler.py", line 100, in handle
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.inference(model_input)
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/89973fac539548c598001685e09e55aa/model_handler.py", line 78, in inference
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.model.forward(**model_input)
2022-06-12T03:00:20,112 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: The following operation failed in the TorchScript interpreter.
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, serialized code (most recent call last):
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__.py", line 29, in forward
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = getattr(hist_future_conv, "0")
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(hist_future_conv, "1")
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future0 = (_0).forward(hist_future, )
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~~~~~~ <--- HERE
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future1 = (_1).forward(hist_future0, )
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_conv = self.hist_conv
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/container.py", line 23, in forward
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(self, "1")
2022-06-12T03:00:20,113 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _2 = getattr(self, "2")
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input0 = (_0).forward(input, )
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -               ~~~~~~~~~~~ <--- HERE
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input1 = (_1).forward(input0, )
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return (_2).forward(input1, )
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 23, in forward
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight = self.weight
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias = self.bias
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = (self)._conv_forward(input, weight, bias, )
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           ~~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _0
2022-06-12T03:00:20,114 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv1d,
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 29, in _conv_forward
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight: Tensor,
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias: Optional[Tensor]) -> Tensor:
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = torch.conv1d(input, weight, bias, [1], "same", [1])
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          ~~~~~~~~~~~~ <--- HERE
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _1
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - 
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, original code (most recent call last):
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/ipykernel_15/297113794.py", line 97, in forward
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         # forward pass through conv blocks
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_future_conv:
2022-06-12T03:00:20,115 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             hist_future = layer(hist_future)
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                           ~~~~~ <--- HERE
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_conv:
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input):
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for module in self:
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             input = module(input)
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~ <--- HERE
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return input
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 302, in forward
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input: Tensor) -> Tensor:
2022-06-12T03:00:20,116 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return self._conv_forward(input, self.weight, self.bias)
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 298, in _conv_forward
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             weight, bias, self.stride,
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             _single(0), self.dilation, self.groups)
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return F.conv1d(input, weight, bias, self.stride,
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~ <--- HERE
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                         self.padding, self.dilation, self.groups)
2022-06-12T03:00:20,117 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: expected scalar type Float but found Double
2022-06-12T03:01:11,963 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:01:11,963 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:02:11,968 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:02:11,968 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:03:11,968 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:03:11,968 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:03:27,947 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:03:27,947 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:03:27,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:03:27,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:03:28,140 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030326550-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:03:28,140 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030326550-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:03:28,150 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030326550-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003006550,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:03:28,150 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030326550-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003006550,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:03:28,157 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030326550-shutdown.cfg
2022-06-12T03:03:28,157 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030326550-shutdown.cfg
2022-06-12T03:03:28,157 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030326550-shutdown.cfg validated successfully
2022-06-12T03:03:28,157 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030326550-shutdown.cfg validated successfully
2022-06-12T03:03:28,243 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:03:28,243 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:03:28,243 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:03:28,243 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:03:28,243 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:03:28,243 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:03:28,244 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:03:28,244 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:03:28,244 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:03:28,244 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:03:28,255 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:03:28,255 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:03:28,255 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:03:28,255 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:03:28,307 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:03:28,307 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:03:28,307 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:03:28,307 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:03:28,308 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:03:28,308 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:03:28,309 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:03:28,309 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:03:28,309 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:03:28,309 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:03:28,444 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:03:28,444 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:03:28,490 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:03:28,490 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:03:28,741 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:03:28,742 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]7021
2022-06-12T03:03:28,742 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:03:28,742 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:03:28,743 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:03:28,743 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:03:28,748 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:03:28,748 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:03:28,756 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:03:28,759 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003008759
2022-06-12T03:03:28,759 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003008759
2022-06-12T03:03:28,791 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:03:30,969 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2177
2022-06-12T03:03:30,969 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2177
2022-06-12T03:03:30,969 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:03:30,969 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:03:30,970 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2720|#Level:Host|#hostname:pop-os,timestamp:1655003010
2022-06-12T03:03:30,970 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:34|#Level:Host|#hostname:pop-os,timestamp:1655003010
2022-06-12T03:03:32,687 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33282 "GET /ping HTTP/1.1" 200 8
2022-06-12T03:03:32,688 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003012
2022-06-12T03:03:34,203 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003014203
2022-06-12T03:03:34,203 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003014203
2022-06-12T03:03:34,205 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003014
2022-06-12T03:03:34,258 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-06-12T03:03:34,258 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-06-12T03:03:34,259 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33284 "POST /predictions/miso_price HTTP/1.1" 503 72
2022-06-12T03:03:34,260 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003012
2022-06-12T03:03:34,261 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 196261, Inference time ns: 58291443
2022-06-12T03:03:34,261 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 196261, Inference time ns: 58291443
2022-06-12T03:03:34,261 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655003014
2022-06-12T03:03:34,263 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T03:03:34,264 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], 'hist': [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], 'tabular': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}
2022-06-12T03:03:34,264 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model: miso_price, Invalid return type: <class 'torch.Tensor'>.
2022-06-12T03:04:28,486 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:04:28,486 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:05:25,570 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:05:25,570 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:05:25,607 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:05:25,607 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:05:25,768 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030524138-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:05:25,768 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030524138-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:05:25,777 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030524138-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003124138,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:05:25,777 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030524138-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003124138,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:05:25,784 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030524138-shutdown.cfg
2022-06-12T03:05:25,784 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030524138-shutdown.cfg
2022-06-12T03:05:25,785 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030524138-shutdown.cfg validated successfully
2022-06-12T03:05:25,785 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030524138-shutdown.cfg validated successfully
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:05:25,870 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:05:25,870 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:05:25,870 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:05:25,878 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:05:25,878 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:05:25,878 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:05:25,878 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:05:25,924 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:05:25,924 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:05:25,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:05:25,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:05:25,925 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:05:25,925 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:05:25,925 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:05:25,925 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:05:25,925 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:05:25,925 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:05:26,058 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:05:26,058 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:05:26,100 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:05:26,100 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:05:26,358 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:05:26,359 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]7306
2022-06-12T03:05:26,360 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:05:26,360 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:05:26,360 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:05:26,360 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:05:26,365 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:05:26,365 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:05:26,374 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:05:26,377 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003126377
2022-06-12T03:05:26,377 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003126377
2022-06-12T03:05:26,407 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:05:28,651 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2244
2022-06-12T03:05:28,651 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2244
2022-06-12T03:05:28,652 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:05:28,652 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:05:28,652 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2778|#Level:Host|#hostname:pop-os,timestamp:1655003128
2022-06-12T03:05:28,653 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:32|#Level:Host|#hostname:pop-os,timestamp:1655003128
2022-06-12T03:05:30,526 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33286 "GET /ping HTTP/1.1" 200 6
2022-06-12T03:05:30,527 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003130
2022-06-12T03:05:31,935 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003131935
2022-06-12T03:05:31,935 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003131935
2022-06-12T03:05:31,937 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003131
2022-06-12T03:05:31,944 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - bytearray(b'"{\\"hist_future\\": [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], \\"hist\\": [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], \\"tabular\\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"')
2022-06-12T03:05:31,946 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': [[[-0.736685342364867, -0.7589273547963183, -0.7681259113598722, -0.7635266330780954, -0.7102334085432178, -0.5936940239324741, -0.5366775741218721, -0.6169337633985965, -0.8935718348654863, -0.9742903854509607, -0.9487388394410878, -0.8275758417428433, -0.6708596928822899, -0.541203847986478, -0.45890353502896386, -0.3985775516399404, -0.32406437651400655, -0.2808700963544596, -0.27218257071110263, -0.21966805995938324, -0.0383737573179046, 0.3397891236282133, 0.5614305341595676, 0.4184392157271744, 0.5447855270445643, 0.5186256108915993, 0.4450128235774419, 0.46263122292139264, 0.4522645956831013, 0.4605384296291553, 0.35791855389236116, 0.08332460610626122, -0.2991455354529495, -0.4979608982154838, -0.6690345824530133, -0.6542390205730106, -0.4570054201825161, -0.23636173668583335, -0.05813361956553954, 0.15331150736758917, 0.2500910297306977, 0.2219843291198376, 0.15676704978035289, 0.048817851590071015, 0.13411134565159888, 0.4213350576082932, 0.49879274422679326, 0.3037979459628782], [-0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.2518456205542737, 0.30847885247215123, 0.2637684062211952, 0.5931353602699053, 0.8926953501513119, 0.6989500830638352, 0.6900079938136436, 0.8852436091094857, 0.888224305526216, 0.6810659045634525, 0.16540542446909165, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, 0.011899559007475708, 0.3919383521406029, 0.5961160566866358, 0.7988034130243036, 0.8673594306091033, 0.8554366449421815, 0.1311274156766921, -0.20122023478874868, -0.2235754579142268, 0.002957469757284565, -0.019397753368193507, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536, -0.6378922598397536], [-0.20551121981177878, -0.5206061447554005, -0.7394790920044225, -0.8436161631900319, -0.8200646376097549, -0.5934373722062597, -0.2208302236725017, 0.29741876754034235, 0.7510979101335685, 1.173644351662159, 1.6108790630389096, 2.029245345817561, 2.406791182894148, 2.742663228251667, 2.9850176195240143, 3.1956621674980092, 3.2840721128147647, 3.285725213069919, 3.1809203058710063, 2.98343872327034, 2.6511367148728264, 2.371820485975279, 1.8355893917375505, 1.1935351465028448, 0.7087892619574109, 0.33658611298725627, 0.06891166743273364, -0.13065587209837443, -0.23427763771816287, -0.2158668004625147, -0.1431551239029262, 0.1514759864108186, 0.5821683783239063, 0.9786279908883813, 1.3667355308420321, 1.7471588341981608, 2.018724867385762, 2.21913338560029, 2.384278513334565, 2.5119835998785573, 2.5831040128508915, 2.6220528687378963, 2.5447075645304467, 2.3646556773881056, 2.0898329130245172, 1.8129241917304837, 1.317204359855329, 0.765501731309393]]], 'hist': [[[10.439999999999998, 9.469999999999999, 9.630000000000003, 8.43, 7.530000000000001, 7.240000000000002, 11.509999999999991, 13.25, 9.890000000000008, 10.420000000000002, 10.969999999999992, 9.920000000000002, 9.14, 10.25, 11.79, 13.650000000000007, 10.660000000000004, 11.880000000000003, 12.939999999999998, 12.86, 11.170000000000009, 7.440000000000005, 2.1900000000000013, -7.339999999999996]]], 'tabular': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}
2022-06-12T03:05:31,996 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 61
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 61
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:05:31,997 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:05:31,998 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33288 "POST /predictions/miso_price HTTP/1.1" 503 78
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003130
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:05:31,999 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 225198, Inference time ns: 63892332
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:05:31,999 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 225198, Inference time ns: 63892332
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:pop-os,timestamp:1655003131
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:05:31,999 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:05:32,000 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]]), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]]), 'tabular': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0, 0, 0, 0, 0]]])}
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T03:05:32,001 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/1d7586b374dc4db3a437bad6dacc9361/model_handler.py", line 103, in handle
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.inference(model_input)
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/1d7586b374dc4db3a437bad6dacc9361/model_handler.py", line 81, in inference
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.model.forward(**model_input)
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: The following operation failed in the TorchScript interpreter.
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, serialized code (most recent call last):
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__.py", line 29, in forward
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = getattr(hist_future_conv, "0")
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(hist_future_conv, "1")
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future0 = (_0).forward(hist_future, )
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~~~~~~ <--- HERE
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future1 = (_1).forward(hist_future0, )
2022-06-12T03:05:32,002 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_conv = self.hist_conv
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/container.py", line 23, in forward
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(self, "1")
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _2 = getattr(self, "2")
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input0 = (_0).forward(input, )
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -               ~~~~~~~~~~~ <--- HERE
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input1 = (_1).forward(input0, )
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return (_2).forward(input1, )
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 23, in forward
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight = self.weight
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias = self.bias
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = (self)._conv_forward(input, weight, bias, )
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           ~~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T03:05:32,003 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _0
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv1d,
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 29, in _conv_forward
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight: Tensor,
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias: Optional[Tensor]) -> Tensor:
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = torch.conv1d(input, weight, bias, [1], "same", [1])
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          ~~~~~~~~~~~~ <--- HERE
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _1
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - 
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, original code (most recent call last):
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/ipykernel_15/297113794.py", line 97, in forward
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         # forward pass through conv blocks
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_future_conv:
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             hist_future = layer(hist_future)
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                           ~~~~~ <--- HERE
2022-06-12T03:05:32,004 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_conv:
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input):
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for module in self:
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             input = module(input)
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~ <--- HERE
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return input
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 302, in forward
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input: Tensor) -> Tensor:
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return self._conv_forward(input, self.weight, self.bias)
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T03:05:32,005 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 298, in _conv_forward
2022-06-12T03:05:32,006 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             weight, bias, self.stride,
2022-06-12T03:05:32,006 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             _single(0), self.dilation, self.groups)
2022-06-12T03:05:32,006 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return F.conv1d(input, weight, bias, self.stride,
2022-06-12T03:05:32,006 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~ <--- HERE
2022-06-12T03:05:32,006 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                         self.padding, self.dilation, self.groups)
2022-06-12T03:05:32,006 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: expected scalar type Float but found Double
2022-06-12T03:06:26,099 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:06:26,099 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:06:55,588 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:06:55,588 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:06:55,626 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:06:55,626 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:06:55,760 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030654485-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:06:55,760 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612030654485-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:06:55,767 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030654485-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003214485,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:06:55,767 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612030654485-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003214485,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:06:55,773 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030654485-shutdown.cfg
2022-06-12T03:06:55,773 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612030654485-shutdown.cfg
2022-06-12T03:06:55,774 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030654485-shutdown.cfg validated successfully
2022-06-12T03:06:55,774 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612030654485-shutdown.cfg validated successfully
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:06:55,858 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:06:55,858 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:06:55,858 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:06:55,866 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:06:55,866 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:06:55,866 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:06:55,866 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:06:55,912 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:06:55,912 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:06:55,912 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:06:55,912 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:06:55,913 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:06:55,913 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:06:55,913 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:06:55,913 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:06:55,914 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:06:55,914 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:06:56,044 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:06:56,044 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:06:56,087 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:06:56,087 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:06:56,319 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:06:56,319 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]7587
2022-06-12T03:06:56,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:06:56,320 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:06:56,320 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:06:56,320 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:06:56,325 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:06:56,325 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:06:56,333 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:06:56,336 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003216336
2022-06-12T03:06:56,336 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003216336
2022-06-12T03:06:56,367 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:06:58,652 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2285
2022-06-12T03:06:58,652 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2285
2022-06-12T03:06:58,653 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:06:58,653 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:06:58,654 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2791|#Level:Host|#hostname:pop-os,timestamp:1655003218
2022-06-12T03:06:58,654 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:33|#Level:Host|#hostname:pop-os,timestamp:1655003218
2022-06-12T03:07:00,050 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33290 "GET /ping HTTP/1.1" 200 7
2022-06-12T03:07:00,051 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003220
2022-06-12T03:07:02,206 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003222206
2022-06-12T03:07:02,206 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003222206
2022-06-12T03:07:02,208 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003222
2022-06-12T03:07:02,267 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:07:02,267 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-06-12T03:07:02,267 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-06-12T03:07:02,268 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:07:02,268 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:07:02,268 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:07:02,268 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:07:02,269 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:07:02,269 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33292 "POST /predictions/miso_price HTTP/1.1" 503 79
2022-06-12T03:07:02,269 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003220
2022-06-12T03:07:02,269 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:07:02,270 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 257589, Inference time ns: 64063632
2022-06-12T03:07:02,270 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 257589, Inference time ns: 64063632
2022-06-12T03:07:02,270 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:07:02,270 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655003222
2022-06-12T03:07:02,270 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:07:02,270 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:07:02,270 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:07:02,270 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:07:02,271 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:07:02,272 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:07:02,272 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:07:02,272 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:07:02,272 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:07:02,272 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:07:02,272 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]]), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]]), 'tabular': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
2022-06-12T03:07:02,273 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0, 0, 0, 0, 0]]])}
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/ts/service.py", line 102, in predict
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-06-12T03:07:02,274 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/83859b8921d04d2ca1dc20173e662d75/model_handler.py", line 103, in handle
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.inference(model_input)
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/models/83859b8921d04d2ca1dc20173e662d75/model_handler.py", line 81, in inference
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     model_output = self.model.forward(**model_input)
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: The following operation failed in the TorchScript interpreter.
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, serialized code (most recent call last):
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__.py", line 29, in forward
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = getattr(hist_future_conv, "0")
2022-06-12T03:07:02,275 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(hist_future_conv, "1")
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future0 = (_0).forward(hist_future, )
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~~~~~~ <--- HERE
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_future1 = (_1).forward(hist_future0, )
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     hist_conv = self.hist_conv
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/container.py", line 23, in forward
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = getattr(self, "1")
2022-06-12T03:07:02,276 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _2 = getattr(self, "2")
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input0 = (_0).forward(input, )
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -               ~~~~~~~~~~~ <--- HERE
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     input1 = (_1).forward(input0, )
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return (_2).forward(input1, )
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 23, in forward
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight = self.weight
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias = self.bias
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _0 = (self)._conv_forward(input, weight, bias, )
2022-06-12T03:07:02,277 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           ~~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _0
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   def _conv_forward(self: __torch__.torch.nn.modules.conv.Conv1d,
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "code/__torch__/torch/nn/modules/conv.py", line 29, in _conv_forward
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     weight: Tensor,
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     bias: Optional[Tensor]) -> Tensor:
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     _1 = torch.conv1d(input, weight, bias, [1], "same", [1])
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          ~~~~~~~~~~~~ <--- HERE
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     return _1
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - 
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Traceback of TorchScript, original code (most recent call last):
2022-06-12T03:07:02,278 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/tmp/ipykernel_15/297113794.py", line 97, in forward
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         # forward pass through conv blocks
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_future_conv:
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             hist_future = layer(hist_future)
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                           ~~~~~ <--- HERE
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for layer in self.hist_conv:
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input):
2022-06-12T03:07:02,279 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         for module in self:
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -             input = module(input)
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                     ~~~~~~ <--- HERE
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return input
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 302, in forward
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -     def forward(self, input: Tensor) -> Tensor:
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return self._conv_forward(input, self.weight, self.bias)
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~~~~~~~~~~~ <--- HERE
2022-06-12T03:07:02,280 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -   File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 298, in _conv_forward
2022-06-12T03:07:02,281 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             weight, bias, self.stride,
2022-06-12T03:07:02,281 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                             _single(0), self.dilation, self.groups)
2022-06-12T03:07:02,281 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -         return F.conv1d(input, weight, bias, self.stride,
2022-06-12T03:07:02,281 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                ~~~~~~~~ <--- HERE
2022-06-12T03:07:02,281 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -                         self.padding, self.dilation, self.groups)
2022-06-12T03:07:02,281 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - RuntimeError: expected scalar type Float but found Double
2022-06-12T03:07:56,086 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:07:56,086 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:08:56,086 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:08:56,086 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:09:56,087 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:09:56,087 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:10:32,511 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:10:32,511 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:10:32,559 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:10:32,559 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:10:32,720 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031031122-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:10:32,720 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031031122-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:10:32,730 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031031122-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003431122,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:10:32,730 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031031122-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003431122,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:10:32,736 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031031122-shutdown.cfg
2022-06-12T03:10:32,736 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031031122-shutdown.cfg
2022-06-12T03:10:32,737 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031031122-shutdown.cfg validated successfully
2022-06-12T03:10:32,737 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031031122-shutdown.cfg validated successfully
2022-06-12T03:10:32,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:10:32,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:10:32,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:10:32,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:10:32,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:10:32,821 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:10:32,822 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:10:32,822 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:10:32,822 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:10:32,822 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:10:32,830 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:10:32,830 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:10:32,830 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:10:32,830 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:10:32,876 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:10:32,876 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:10:32,876 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:10:32,876 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:10:32,877 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:10:32,877 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:10:32,877 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:10:32,877 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:10:32,878 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:10:32,878 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:10:33,008 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:10:33,008 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:10:33,051 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:10:33,051 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:10:33,302 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:10:33,302 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]7871
2022-06-12T03:10:33,303 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:10:33,303 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:10:33,304 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:10:33,304 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:10:33,307 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:10:33,307 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:10:33,316 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:10:33,318 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003433318
2022-06-12T03:10:33,318 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003433318
2022-06-12T03:10:33,352 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:10:35,524 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2172
2022-06-12T03:10:35,524 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2172
2022-06-12T03:10:35,525 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:10:35,525 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:10:35,525 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2699|#Level:Host|#hostname:pop-os,timestamp:1655003435
2022-06-12T03:10:35,526 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:36|#Level:Host|#hostname:pop-os,timestamp:1655003435
2022-06-12T03:10:42,966 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33296 "GET /ping HTTP/1.1" 200 6
2022-06-12T03:10:42,966 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003442
2022-06-12T03:10:44,631 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003444631
2022-06-12T03:10:44,631 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003444631
2022-06-12T03:10:44,633 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003444
2022-06-12T03:10:44,684 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:10:44,684 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-06-12T03:10:44,684 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:10:44,684 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-06-12T03:10:44,684 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:10:44,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:10:44,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:10:44,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:10:44,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:10:44,685 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:10:44,686 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:10:44,686 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:10:44,686 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33298 "POST /predictions/miso_price HTTP/1.1" 503 70
2022-06-12T03:10:44,686 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003442
2022-06-12T03:10:44,686 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:10:44,687 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218887, Inference time ns: 56080127
2022-06-12T03:10:44,687 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:10:44,687 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218887, Inference time ns: 56080127
2022-06-12T03:10:44,687 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:10:44,687 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655003444
2022-06-12T03:10:44,687 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:10:44,688 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:10:44,688 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:10:44,688 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:10:44,688 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:10:44,688 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:10:44,688 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:10:44,689 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:10:44,689 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:10:44,689 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:10:44,689 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]], dtype=torch.float64), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:10:44,690 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]], dtype=torch.float64), 'tabular': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)}
2022-06-12T03:10:44,691 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model: miso_price, Invalid return type: <class 'torch.Tensor'>.
2022-06-12T03:11:33,052 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:11:33,052 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:12:33,049 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:12:33,049 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:13:33,050 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:13:33,050 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:14:15,315 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:14:15,315 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:14:15,353 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:14:15,353 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:14:15,496 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031413835-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:14:15,496 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031413835-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:14:15,506 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031413835-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003653835,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:14:15,506 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031413835-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003653835,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:14:15,512 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031413835-shutdown.cfg
2022-06-12T03:14:15,512 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031413835-shutdown.cfg
2022-06-12T03:14:15,512 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031413835-shutdown.cfg validated successfully
2022-06-12T03:14:15,512 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031413835-shutdown.cfg validated successfully
2022-06-12T03:14:15,595 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:14:15,595 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:14:15,596 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:14:15,596 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:14:15,596 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:14:15,596 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:14:15,596 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:14:15,596 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:14:15,596 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:14:15,596 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:14:15,603 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:14:15,603 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:14:15,603 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:14:15,603 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:14:15,648 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:14:15,648 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:14:15,649 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:14:15,649 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:14:15,650 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:14:15,650 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:14:15,650 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:14:15,650 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:14:15,650 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:14:15,650 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:14:15,779 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:14:15,779 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:14:15,823 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:14:15,823 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:14:16,075 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:14:16,076 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]8158
2022-06-12T03:14:16,076 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:14:16,076 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:14:16,077 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:14:16,077 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:14:16,082 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:14:16,082 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:14:16,091 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:14:16,093 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003656093
2022-06-12T03:14:16,093 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003656093
2022-06-12T03:14:16,126 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:14:18,403 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2278
2022-06-12T03:14:18,403 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2278
2022-06-12T03:14:18,404 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:14:18,404 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:14:18,405 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2805|#Level:Host|#hostname:pop-os,timestamp:1655003658
2022-06-12T03:14:18,405 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:34|#Level:Host|#hostname:pop-os,timestamp:1655003658
2022-06-12T03:14:20,932 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33300 "GET /ping HTTP/1.1" 200 6
2022-06-12T03:14:20,933 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003660
2022-06-12T03:14:22,806 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003662806
2022-06-12T03:14:22,806 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003662806
2022-06-12T03:14:22,807 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003662
2022-06-12T03:14:22,858 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:14:22,858 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-06-12T03:14:22,858 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-06-12T03:14:22,858 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:14:22,858 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:14:22,859 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:14:22,859 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:14:22,859 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33302 "POST /predictions/miso_price HTTP/1.1" 503 68
2022-06-12T03:14:22,859 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:14:22,860 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003660
2022-06-12T03:14:22,860 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:14:22,860 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 194221, Inference time ns: 54058853
2022-06-12T03:14:22,860 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 194221, Inference time ns: 54058853
2022-06-12T03:14:22,860 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:pop-os,timestamp:1655003662
2022-06-12T03:14:22,860 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:14:22,860 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:14:22,860 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:14:22,861 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:14:22,861 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:14:22,861 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:14:22,861 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:14:22,862 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:14:22,862 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:14:22,862 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:14:22,862 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:14:22,862 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:14:22,863 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:14:22,863 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:14:22,863 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:14:22,863 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:14:22,864 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:14:22,864 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:14:22,864 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:14:22,864 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:14:22,864 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:14:22,865 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:14:22,865 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]], dtype=torch.float64), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:14:22,865 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:14:22,865 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:14:22,865 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]], dtype=torch.float64), 'tabular': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:14:22,865 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:14:22,866 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
2022-06-12T03:14:22,866 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:14:22,866 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)}
2022-06-12T03:14:22,866 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model: miso_price, Invalid return type: <class 'dict'>.
2022-06-12T03:15:15,819 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:15:15,819 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:16:15,817 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:16:15,817 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:16:17,798 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:16:17,798 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:16:17,837 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:16:17,837 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:16:17,984 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031616031-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:16:17,984 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031616031-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:16:17,992 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031616031-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003776031,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:16:17,992 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031616031-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003776031,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:16:18,000 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031616031-shutdown.cfg
2022-06-12T03:16:18,000 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031616031-shutdown.cfg
2022-06-12T03:16:18,000 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031616031-shutdown.cfg validated successfully
2022-06-12T03:16:18,000 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031616031-shutdown.cfg validated successfully
2022-06-12T03:16:18,084 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:16:18,084 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:16:18,085 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:16:18,085 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:16:18,085 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:16:18,085 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:16:18,085 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:16:18,085 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:16:18,085 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:16:18,085 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:16:18,094 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:16:18,094 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:16:18,094 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:16:18,094 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:16:18,144 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:16:18,144 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:16:18,144 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:16:18,144 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:16:18,145 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:16:18,145 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:16:18,146 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:16:18,146 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:16:18,146 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:16:18,146 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:16:18,284 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:16:18,284 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:16:18,326 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:16:18,326 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:16:18,552 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:16:18,553 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]8445
2022-06-12T03:16:18,554 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:16:18,554 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:16:18,554 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:16:18,554 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:16:18,557 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:16:18,557 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:16:18,565 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:16:18,568 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003778567
2022-06-12T03:16:18,568 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003778567
2022-06-12T03:16:18,599 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:16:20,854 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2255
2022-06-12T03:16:20,854 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2255
2022-06-12T03:16:20,855 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:16:20,855 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:16:20,855 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2766|#Level:Host|#hostname:pop-os,timestamp:1655003780
2022-06-12T03:16:20,856 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:34|#Level:Host|#hostname:pop-os,timestamp:1655003780
2022-06-12T03:16:25,427 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33304 "GET /ping HTTP/1.1" 200 7
2022-06-12T03:16:25,428 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003785
2022-06-12T03:16:27,232 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003787232
2022-06-12T03:16:27,232 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003787232
2022-06-12T03:16:27,234 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003787
2022-06-12T03:16:27,286 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:16:27,287 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-06-12T03:16:27,287 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-06-12T03:16:27,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:16:27,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:16:27,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:16:27,287 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:16:27,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:16:27,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:16:27,288 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:16:27,288 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33306 "POST /predictions/miso_price HTTP/1.1" 503 71
2022-06-12T03:16:27,289 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:16:27,289 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003785
2022-06-12T03:16:27,289 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:16:27,289 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 208368, Inference time ns: 56886625
2022-06-12T03:16:27,289 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:16:27,289 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 208368, Inference time ns: 56886625
2022-06-12T03:16:27,289 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:16:27,289 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:pop-os,timestamp:1655003787
2022-06-12T03:16:27,290 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:16:27,290 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:16:27,290 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:16:27,291 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:16:27,291 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:16:27,291 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:16:27,291 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:16:27,291 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:16:27,291 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:16:27,292 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:16:27,292 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:16:27,292 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:16:27,292 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:16:27,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:16:27,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:16:27,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:16:27,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:16:27,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]], dtype=torch.float64), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:16:27,293 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]], dtype=torch.float64), 'tabular': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)}
2022-06-12T03:16:27,294 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model: miso_price, Invalid return type: <class 'str'>.
2022-06-12T03:17:18,323 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:17:18,323 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:18:18,323 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:18:18,323 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:18:54,369 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:18:54,369 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:18:54,409 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:18:54,409 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:18:54,572 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031853337-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:18:54,572 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612031853337-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:18:54,579 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031853337-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003933338,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:18:54,579 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612031853337-shutdown.cfg",
  "modelCount": 1,
  "created": 1655003933338,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:18:54,585 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031853337-shutdown.cfg
2022-06-12T03:18:54,585 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612031853337-shutdown.cfg
2022-06-12T03:18:54,585 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031853337-shutdown.cfg validated successfully
2022-06-12T03:18:54,585 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612031853337-shutdown.cfg validated successfully
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:18:54,670 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:18:54,670 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:18:54,670 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:18:54,678 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:18:54,678 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:18:54,678 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:18:54,678 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:18:54,727 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:18:54,727 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:18:54,727 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:18:54,727 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:18:54,728 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:18:54,728 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:18:54,728 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:18:54,728 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:18:54,729 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:18:54,729 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:18:54,866 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:18:54,866 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:18:54,910 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:18:54,910 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:18:55,149 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:18:55,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]8727
2022-06-12T03:18:55,150 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:18:55,151 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:18:55,151 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:18:55,151 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:18:55,156 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:18:55,156 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:18:55,171 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:18:55,177 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003935177
2022-06-12T03:18:55,177 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003935177
2022-06-12T03:18:55,196 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:18:57,423 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2228
2022-06-12T03:18:57,423 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2228
2022-06-12T03:18:57,423 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:18:57,423 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:18:57,424 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2750|#Level:Host|#hostname:pop-os,timestamp:1655003937
2022-06-12T03:18:57,424 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:19|#Level:Host|#hostname:pop-os,timestamp:1655003937
2022-06-12T03:19:00,718 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33308 "GET /ping HTTP/1.1" 200 7
2022-06-12T03:19:00,718 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003940
2022-06-12T03:19:02,146 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003942146
2022-06-12T03:19:02,146 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655003942146
2022-06-12T03:19:02,148 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655003942
2022-06-12T03:19:02,207 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:19:02,207 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-06-12T03:19:02,207 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-06-12T03:19:02,208 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:19:02,208 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:19:02,208 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:19:02,208 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33310 "POST /predictions/miso_price HTTP/1.1" 200 78
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003940
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:19:02,209 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 211635, Backend time ns: 62608755
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:19:02,209 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 211635, Backend time ns: 62608755
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:pop-os,timestamp:1655003942
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:pop-os,timestamp:1655003942
2022-06-12T03:19:02,209 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:19:02,210 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:19:02,210 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:19:02,210 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:19:02,210 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:19:02,210 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:19:02,211 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:19:02,211 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:19:02,211 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:19:02,211 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:19:02,211 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:19:02,211 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:19:02,212 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:19:02,212 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:19:02,212 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:19:02,212 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:19:02,212 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:19:02,213 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:19:02,213 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:19:02,213 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:19:02,213 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:19:02,213 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]], dtype=torch.float64), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:19:02,213 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:19:02,214 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:19:02,214 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]], dtype=torch.float64), 'tabular': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:19:02,214 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:19:02,214 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
2022-06-12T03:19:02,214 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:19:02,214 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)}
2022-06-12T03:19:02,215 [INFO ] W-9000-miso_price_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:58.87|#ModelName:miso_price,Level:Model|#hostname:pop-os,requestID:179fb92c-de0e-40d4-8a6e-1207b1719559,timestamp:1655003942
2022-06-12T03:19:54,906 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:19:54,906 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:20:54,906 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:20:54,906 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:21:25,380 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655004085380
2022-06-12T03:21:25,380 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655004085380
2022-06-12T03:21:25,382 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655004085
2022-06-12T03:21:25,413 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:21:25,413 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 32
2022-06-12T03:21:25,413 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 32
2022-06-12T03:21:25,413 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:21:25,413 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33312 "POST /predictions/miso_price HTTP/1.1" 200 41
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655003940
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:21:25,414 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 135419, Backend time ns: 33610880
2022-06-12T03:21:25,414 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 135419, Backend time ns: 33610880
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:pop-os,timestamp:1655004085
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:pop-os,timestamp:1655004085
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:21:25,414 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:21:25,415 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]], dtype=torch.float64), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]], dtype=torch.float64), 'tabular': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)}
2022-06-12T03:21:25,416 [INFO ] W-9000-miso_price_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:30.89|#ModelName:miso_price,Level:Model|#hostname:pop-os,requestID:86c2acb1-5664-4c1d-840f-f6c2fa9ca835,timestamp:1655004085
2022-06-12T03:21:54,908 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:21:54,908 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:22:54,906 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:22:54,906 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:23:54,907 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:23:54,907 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:24:54,904 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:24:54,904 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:25:54,908 [ERROR] Thread-8 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:25:54,908 [ERROR] Thread-8 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:26:54,905 [ERROR] Thread-9 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:26:54,905 [ERROR] Thread-9 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:27:32,781 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:27:32,781 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-06-12T03:27:32,821 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:27:32,821 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-06-12T03:27:32,948 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612032730211-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:27:32,948 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /workspace
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 48
Max heap size: 20088 M
Python executable: /opt/conda/bin/python3
Config file: logs/config/20220612032730211-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /workspace/models
Initial Models: miso_price=pytorch_cnn.mar
Log dir: /workspace/logs
Metrics dir: /workspace/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /workspace/models
Model config: N/A
2022-06-12T03:27:32,956 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612032730211-shutdown.cfg",
  "modelCount": 1,
  "created": 1655004450212,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:27:32,956 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220612032730211-shutdown.cfg",
  "modelCount": 1,
  "created": 1655004450212,
  "models": {
    "miso_price": {
      "1.0": {
        "defaultVersion": true,
        "marName": "pytorch_cnn.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-06-12T03:27:32,963 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612032730211-shutdown.cfg
2022-06-12T03:27:32,963 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220612032730211-shutdown.cfg
2022-06-12T03:27:32,964 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612032730211-shutdown.cfg validated successfully
2022-06-12T03:27:32,964 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220612032730211-shutdown.cfg validated successfully
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model miso_price
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model miso_price
2022-06-12T03:27:33,049 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:27:33,049 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model miso_price loaded.
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:27:33,049 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: miso_price, count: 1
2022-06-12T03:27:33,058 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:27:33,058 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-06-12T03:27:33,057 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:27:33,057 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3, /opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-06-12T03:27:33,111 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:27:33,111 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-06-12T03:27:33,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:27:33,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-06-12T03:27:33,113 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:27:33,113 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-06-12T03:27:33,113 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:27:33,113 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-06-12T03:27:33,113 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:27:33,113 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-06-12T03:27:33,243 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:27:33,243 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-06-12T03:27:33,284 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:27:33,284 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:27:33,534 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-06-12T03:27:33,535 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [PID]9036
2022-06-12T03:27:33,535 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Torch worker started.
2022-06-12T03:27:33,536 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Python runtime: 3.8.12
2022-06-12T03:27:33,536 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:27:33,536 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change null -> WORKER_STARTED
2022-06-12T03:27:33,541 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:27:33,541 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-06-12T03:27:33,549 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-06-12T03:27:33,551 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655004453551
2022-06-12T03:27:33,551 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655004453551
2022-06-12T03:27:33,585 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - model_name: miso_price, batchSize: 1
2022-06-12T03:27:35,850 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2266
2022-06-12T03:27:35,850 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2266
2022-06-12T03:27:35,851 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:27:35,851 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-miso_price_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-06-12T03:27:35,851 [INFO ] W-9000-miso_price_1.0 TS_METRICS - W-9000-miso_price_1.0.ms:2798|#Level:Host|#hostname:pop-os,timestamp:1655004455
2022-06-12T03:27:35,852 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:pop-os,timestamp:1655004455
2022-06-12T03:27:38,208 [INFO ] pool-2-thread-2 ACCESS_LOG - /127.0.0.1:33316 "GET /ping HTTP/1.1" 200 7
2022-06-12T03:27:38,208 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655004458
2022-06-12T03:27:40,549 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655004460549
2022-06-12T03:27:40,549 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1655004460549
2022-06-12T03:27:40,551 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - Backend received inference at: 1655004460
2022-06-12T03:27:40,606 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - {'hist_future': tensor([[[-7.3669e-01, -7.5893e-01, -7.6813e-01, -7.6353e-01, -7.1023e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9369e-01, -5.3668e-01, -6.1693e-01, -8.9357e-01, -9.7429e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -9.4874e-01, -8.2758e-01, -6.7086e-01, -5.4120e-01, -4.5890e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.9858e-01, -3.2406e-01, -2.8087e-01, -2.7218e-01, -2.1967e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -3.8374e-02,  3.3979e-01,  5.6143e-01,  4.1844e-01,  5.4479e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            5.1863e-01,  4.4501e-01,  4.6263e-01,  4.5226e-01,  4.6054e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.5792e-01,  8.3325e-02, -2.9915e-01, -4.9796e-01, -6.6903e-01,
2022-06-12T03:27:40,607 [INFO ] W-9000-miso_price_1.0 ACCESS_LOG - /127.0.0.1:33318 "POST /predictions/miso_price HTTP/1.1" 200 73
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.5424e-01, -4.5701e-01, -2.3636e-01, -5.8134e-02,  1.5331e-01,
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:pop-os,timestamp:1655004458
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5009e-01,  2.2198e-01,  1.5677e-01,  4.8818e-02,  1.3411e-01,
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            4.2134e-01,  4.9879e-01,  3.0380e-01],
2022-06-12T03:27:40,608 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 185906, Backend time ns: 58810775
2022-06-12T03:27:40,608 [DEBUG] W-9000-miso_price_1.0 org.pytorch.serve.job.Job - Waiting time ns: 185906, Backend time ns: 58810775
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:pop-os,timestamp:1655004460
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01,  2.5185e-01,  3.0848e-01,
2022-06-12T03:27:40,608 [INFO ] W-9000-miso_price_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:pop-os,timestamp:1655004460
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6377e-01,  5.9314e-01,  8.9270e-01,  6.9895e-01,  6.9001e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            8.8524e-01,  8.8822e-01,  6.8107e-01,  1.6541e-01, -6.3789e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01, -6.3789e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01,  1.1900e-02,  3.9194e-01,  5.9612e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.9880e-01,  8.6736e-01,  8.5544e-01,  1.3113e-01, -2.0122e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -2.2358e-01,  2.9575e-03, -1.9398e-02, -6.3789e-01, -6.3789e-01,
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -6.3789e-01, -6.3789e-01, -6.3789e-01],
2022-06-12T03:27:40,609 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -          [-2.0551e-01, -5.2061e-01, -7.3948e-01, -8.4362e-01, -8.2006e-01,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -5.9344e-01, -2.2083e-01,  2.9742e-01,  7.5110e-01,  1.1736e+00,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.6109e+00,  2.0292e+00,  2.4068e+00,  2.7427e+00,  2.9850e+00,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.1957e+00,  3.2841e+00,  3.2857e+00,  3.1809e+00,  2.9834e+00,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.6511e+00,  2.3718e+00,  1.8356e+00,  1.1935e+00,  7.0879e-01,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            3.3659e-01,  6.8912e-02, -1.3066e-01, -2.3428e-01, -2.1587e-01,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           -1.4316e-01,  1.5148e-01,  5.8217e-01,  9.7863e-01,  1.3667e+00,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.7472e+00,  2.0187e+00,  2.2191e+00,  2.3843e+00,  2.5120e+00,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            2.5831e+00,  2.6221e+00,  2.5447e+00,  2.3647e+00,  2.0898e+00,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            1.8129e+00,  1.3172e+00,  7.6550e-01]]], dtype=torch.float64), 'hist': tensor([[[10.4400,  9.4700,  9.6300,  8.4300,  7.5300,  7.2400, 11.5100,
2022-06-12T03:27:40,610 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           13.2500,  9.8900, 10.4200, 10.9700,  9.9200,  9.1400, 10.2500,
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           11.7900, 13.6500, 10.6600, 11.8800, 12.9400, 12.8600, 11.1700,
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -            7.4400,  2.1900, -7.3400]]], dtype=torch.float64), 'tabular': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG -           0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)}
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - output
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_LOG - [[-3.0815003968813346, -2.661775357506681, -1.8178052570329999, -1.512483823879277, -1.31611529680596, -0.8093887772397644, -0.668044658003574, -0.8107391762159403, -0.8144769099744404, 0.4177080906225864, 0.7938152127647358, 0.4109675611114337, -0.17333705714298528, 0.316888608121709, 0.6764291151911058, 3.0014238118347394, 5.908368267283859, 7.755606693778795, 9.299041227017362, 9.49517364052904, 7.574118949324538, 4.933762952600338, 3.351720706429167, 2.1533193683194307]]
2022-06-12T03:27:40,611 [INFO ] W-9000-miso_price_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.68|#ModelName:miso_price,Level:Model|#hostname:pop-os,requestID:9a407304-0c76-4b2e-b7c8-ac26e2d6b6aa,timestamp:1655004460
2022-06-12T03:28:33,283 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:28:33,283 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:29:33,283 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2022-06-12T03:29:33,283 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 91, in collect_all
    value(num_of_gpu)
  File "/opt/conda/lib/python3.8/site-packages/ts/metrics/system_metrics.py", line 61, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

